{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: José Jesús La Casa Nieto\n",
        "*   Alumno 2:\n",
        "*   Alumno 3:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I6n7MIefJ21i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos en el directorio: \n",
            "['Proyecto_Grupo6.ipynb', 'Proyecto_práctico.ipynb']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbVRjvHCJ8UF"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jwOE6I_KGb2a"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jGEZUcpGb2a"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de acciones disponibles: 6\n"
          ]
        }
      ],
      "source": [
        "print(\"Número de acciones disponibles: \" + str(nb_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formato de observaciones: Box(0, 255, (210, 160, 3), uint8)\n"
          ]
        }
      ],
      "source": [
        "print(\"Formato de observaciones:\", env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O4GKrfWSGb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "channels_last\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5184)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               2654720   \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 774       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 2,762,214\n",
            "Trainable params: 2,762,214\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "print(K.image_data_format())\n",
        "\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "elif K.image_data_format() == 'channels_first':\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "else:\n",
        "    raise RuntimeError('Unknown image_dim_ordering.')\n",
        "\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "foSlxWH1Gb2b"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = LinearAnnealedPolicy(\n",
        "    EpsGreedyQPolicy(),\n",
        "    attr='eps',\n",
        "    value_max=1.,\n",
        "    value_min=.1,\n",
        "    value_test=.05,\n",
        "    nb_steps=1000000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "dqn = DQNAgent(\n",
        "    model=model,\n",
        "    nb_actions=nb_actions,\n",
        "    policy=policy,\n",
        "    memory=memory,\n",
        "    processor=processor,\n",
        "    nb_steps_warmup=50000,\n",
        "    gamma=.99,\n",
        "    target_model_update=10000,\n",
        "    train_interval=4\n",
        ")\n",
        "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 500000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pepel\\.conda\\envs\\Miar_rl38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 5s 2ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 2.137\n",
            "\n",
            "Interval 2 (1000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 8.500 [7.000, 10.000] - ale.lives: 2.242\n",
            "\n",
            "Interval 3 (2000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - ale.lives: 2.130\n",
            "\n",
            "Interval 4 (3000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - ale.lives: 1.843\n",
            "\n",
            "Interval 5 (4000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - ale.lives: 1.897\n",
            "\n",
            "Interval 6 (5000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - ale.lives: 2.005\n",
            "\n",
            "Interval 7 (6000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 1.833\n",
            "\n",
            "Interval 8 (7000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0070\n",
            "2 episodes - episode_reward: 7.000 [2.000, 12.000] - ale.lives: 2.283\n",
            "\n",
            "Interval 9 (8000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 1.740\n",
            "\n",
            "Interval 10 (9000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - ale.lives: 1.781\n",
            "\n",
            "Interval 11 (10000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - ale.lives: 2.344\n",
            "\n",
            "Interval 12 (11000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - ale.lives: 2.203\n",
            "\n",
            "Interval 13 (12000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - ale.lives: 2.165\n",
            "\n",
            "Interval 14 (13000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.000 [5.000, 13.000] - ale.lives: 2.126\n",
            "\n",
            "Interval 15 (14000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 6.000 [3.000, 9.000] - ale.lives: 2.147\n",
            "\n",
            "Interval 16 (15000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - ale.lives: 2.290\n",
            "\n",
            "Interval 17 (16000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - ale.lives: 2.251\n",
            "\n",
            "Interval 18 (17000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 2.175\n",
            "\n",
            "Interval 19 (18000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - ale.lives: 2.048\n",
            "\n",
            "Interval 20 (19000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 10.500 [10.000, 11.000] - ale.lives: 1.966\n",
            "\n",
            "Interval 21 (20000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 1.796\n",
            "\n",
            "Interval 22 (21000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - ale.lives: 2.070\n",
            "\n",
            "Interval 23 (22000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 2.036\n",
            "\n",
            "Interval 24 (23000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0100\n",
            "Interval 25 (24000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.500 [14.000, 15.000] - ale.lives: 2.176\n",
            "\n",
            "Interval 26 (25000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - ale.lives: 2.246\n",
            "\n",
            "Interval 27 (26000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0070\n",
            "2 episodes - episode_reward: 5.000 [3.000, 7.000] - ale.lives: 1.923\n",
            "\n",
            "Interval 28 (27000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 2.105\n",
            "\n",
            "Interval 29 (28000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 7.000 [7.000, 7.000] - ale.lives: 2.262\n",
            "\n",
            "Interval 30 (29000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 2.187\n",
            "\n",
            "Interval 31 (30000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - ale.lives: 2.111\n",
            "\n",
            "Interval 32 (31000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - ale.lives: 2.260\n",
            "\n",
            "Interval 33 (32000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0080\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - ale.lives: 1.618\n",
            "\n",
            "Interval 34 (33000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 2.321\n",
            "\n",
            "Interval 35 (34000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 13.500 [12.000, 15.000] - ale.lives: 2.020\n",
            "\n",
            "Interval 36 (35000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 2.344\n",
            "\n",
            "Interval 37 (36000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - ale.lives: 1.948\n",
            "\n",
            "Interval 38 (37000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - ale.lives: 2.102\n",
            "\n",
            "Interval 39 (38000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 2.213\n",
            "\n",
            "Interval 40 (39000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - ale.lives: 2.118\n",
            "\n",
            "Interval 41 (40000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 3.000 [2.000, 4.000] - ale.lives: 2.272\n",
            "\n",
            "Interval 42 (41000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - ale.lives: 2.495\n",
            "\n",
            "Interval 43 (42000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - ale.lives: 2.191\n",
            "\n",
            "Interval 44 (43000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 2.264\n",
            "\n",
            "Interval 45 (44000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - ale.lives: 2.307\n",
            "\n",
            "Interval 46 (45000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.000 [8.000, 18.000] - ale.lives: 1.817\n",
            "\n",
            "Interval 47 (46000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 1.519\n",
            "\n",
            "Interval 48 (47000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "Interval 49 (48000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 2.322\n",
            "\n",
            "Interval 50 (49000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - ale.lives: 1.657\n",
            "\n",
            "Interval 51 (50000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.007 - mae: 0.057 - mean_q: 0.081 - mean_eps: 0.955 - ale.lives: 1.939\n",
            "\n",
            "Interval 52 (51000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.006 - mae: 0.057 - mean_q: 0.078 - mean_eps: 0.954 - ale.lives: 2.051\n",
            "\n",
            "Interval 53 (52000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 5.500 [3.000, 8.000] - loss: 0.006 - mae: 0.057 - mean_q: 0.076 - mean_eps: 0.953 - ale.lives: 2.469\n",
            "\n",
            "Interval 54 (53000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.007 - mae: 0.059 - mean_q: 0.077 - mean_eps: 0.952 - ale.lives: 2.108\n",
            "\n",
            "Interval 55 (54000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.006 - mae: 0.057 - mean_q: 0.076 - mean_eps: 0.951 - ale.lives: 2.162\n",
            "\n",
            "Interval 56 (55000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.007 - mae: 0.059 - mean_q: 0.074 - mean_eps: 0.950 - ale.lives: 1.582\n",
            "\n",
            "Interval 57 (56000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.007 - mae: 0.059 - mean_q: 0.076 - mean_eps: 0.949 - ale.lives: 1.981\n",
            "\n",
            "Interval 58 (57000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.061 - mean_q: 0.079 - mean_eps: 0.948 - ale.lives: 2.144\n",
            "\n",
            "Interval 59 (58000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - loss: 0.005 - mae: 0.056 - mean_q: 0.073 - mean_eps: 0.947 - ale.lives: 2.404\n",
            "\n",
            "Interval 60 (59000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.006 - mae: 0.056 - mean_q: 0.074 - mean_eps: 0.946 - ale.lives: 1.429\n",
            "\n",
            "Interval 61 (60000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.007 - mae: 0.071 - mean_q: 0.093 - mean_eps: 0.946 - ale.lives: 1.976\n",
            "\n",
            "Interval 62 (61000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.093 - mean_eps: 0.945 - ale.lives: 2.528\n",
            "\n",
            "Interval 63 (62000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [6.000, 18.000] - loss: 0.006 - mae: 0.071 - mean_q: 0.091 - mean_eps: 0.944 - ale.lives: 1.943\n",
            "\n",
            "Interval 64 (63000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.006 - mae: 0.072 - mean_q: 0.092 - mean_eps: 0.943 - ale.lives: 2.252\n",
            "\n",
            "Interval 65 (64000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.093 - mean_eps: 0.942 - ale.lives: 1.892\n",
            "\n",
            "Interval 66 (65000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.006 - mae: 0.072 - mean_q: 0.093 - mean_eps: 0.941 - ale.lives: 2.429\n",
            "\n",
            "Interval 67 (66000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 3.500 [3.000, 4.000] - loss: 0.006 - mae: 0.072 - mean_q: 0.092 - mean_eps: 0.940 - ale.lives: 2.191\n",
            "\n",
            "Interval 68 (67000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "Interval 69 (68000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 15.000 [10.000, 20.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.095 - mean_eps: 0.938 - ale.lives: 2.279\n",
            "\n",
            "Interval 70 (69000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.006 - mae: 0.073 - mean_q: 0.094 - mean_eps: 0.937 - ale.lives: 2.313\n",
            "\n",
            "Interval 71 (70000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.006 - mae: 0.082 - mean_q: 0.105 - mean_eps: 0.937 - ale.lives: 1.963\n",
            "\n",
            "Interval 72 (71000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0080\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 0.082 - mean_q: 0.108 - mean_eps: 0.936 - ale.lives: 2.292\n",
            "\n",
            "Interval 73 (72000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.000 [1.000, 15.000] - loss: 0.007 - mae: 0.084 - mean_q: 0.109 - mean_eps: 0.935 - ale.lives: 1.799\n",
            "\n",
            "Interval 74 (73000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.007 - mae: 0.085 - mean_q: 0.109 - mean_eps: 0.934 - ale.lives: 2.257\n",
            "\n",
            "Interval 75 (74000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 0.082 - mean_q: 0.106 - mean_eps: 0.933 - ale.lives: 2.378\n",
            "\n",
            "Interval 76 (75000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.007 - mae: 0.084 - mean_q: 0.110 - mean_eps: 0.932 - ale.lives: 2.058\n",
            "\n",
            "Interval 77 (76000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 7.500 [4.000, 11.000] - loss: 0.007 - mae: 0.084 - mean_q: 0.108 - mean_eps: 0.931 - ale.lives: 1.996\n",
            "\n",
            "Interval 78 (77000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.006 - mae: 0.082 - mean_q: 0.106 - mean_eps: 0.930 - ale.lives: 2.380\n",
            "\n",
            "Interval 79 (78000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.007 - mae: 0.084 - mean_q: 0.110 - mean_eps: 0.929 - ale.lives: 2.095\n",
            "\n",
            "Interval 80 (79000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.007 - mae: 0.085 - mean_q: 0.108 - mean_eps: 0.928 - ale.lives: 1.992\n",
            "\n",
            "Interval 81 (80000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.006 - mae: 0.089 - mean_q: 0.114 - mean_eps: 0.928 - ale.lives: 1.785\n",
            "\n",
            "Interval 82 (81000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.007 - mae: 0.095 - mean_q: 0.124 - mean_eps: 0.927 - ale.lives: 2.028\n",
            "\n",
            "Interval 83 (82000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 0.092 - mean_q: 0.120 - mean_eps: 0.926 - ale.lives: 2.382\n",
            "\n",
            "Interval 84 (83000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.007 - mae: 0.094 - mean_q: 0.120 - mean_eps: 0.925 - ale.lives: 2.150\n",
            "\n",
            "Interval 85 (84000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 5.500 [5.000, 6.000] - loss: 0.007 - mae: 0.094 - mean_q: 0.124 - mean_eps: 0.924 - ale.lives: 2.143\n",
            "\n",
            "Interval 86 (85000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.095 - mean_q: 0.124 - mean_eps: 0.923 - ale.lives: 2.395\n",
            "\n",
            "Interval 87 (86000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.006 - mae: 0.093 - mean_q: 0.121 - mean_eps: 0.922 - ale.lives: 1.745\n",
            "\n",
            "Interval 88 (87000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.007 - mae: 0.095 - mean_q: 0.122 - mean_eps: 0.921 - ale.lives: 2.352\n",
            "\n",
            "Interval 89 (88000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 5.500 [5.000, 6.000] - loss: 0.006 - mae: 0.093 - mean_q: 0.122 - mean_eps: 0.920 - ale.lives: 2.137\n",
            "\n",
            "Interval 90 (89000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.007 - mae: 0.094 - mean_q: 0.124 - mean_eps: 0.919 - ale.lives: 1.986\n",
            "\n",
            "Interval 91 (90000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.006 - mae: 0.112 - mean_q: 0.145 - mean_eps: 0.919 - ale.lives: 2.277\n",
            "\n",
            "Interval 92 (91000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.007 - mae: 0.115 - mean_q: 0.148 - mean_eps: 0.918 - ale.lives: 1.851\n",
            "\n",
            "Interval 93 (92000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.008 - mae: 0.118 - mean_q: 0.156 - mean_eps: 0.917 - ale.lives: 2.178\n",
            "\n",
            "Interval 94 (93000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0080\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.007 - mae: 0.116 - mean_q: 0.151 - mean_eps: 0.916 - ale.lives: 2.011\n",
            "\n",
            "Interval 95 (94000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.007 - mae: 0.117 - mean_q: 0.154 - mean_eps: 0.915 - ale.lives: 1.967\n",
            "\n",
            "Interval 96 (95000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.117 - mean_q: 0.154 - mean_eps: 0.914 - ale.lives: 2.350\n",
            "\n",
            "Interval 97 (96000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.006 - mae: 0.113 - mean_q: 0.147 - mean_eps: 0.913 - ale.lives: 2.096\n",
            "\n",
            "Interval 98 (97000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0040\n",
            "2 episodes - episode_reward: 3.500 [3.000, 4.000] - loss: 0.006 - mae: 0.114 - mean_q: 0.147 - mean_eps: 0.912 - ale.lives: 1.868\n",
            "\n",
            "Interval 99 (98000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0070\n",
            "2 episodes - episode_reward: 3.500 [3.000, 4.000] - loss: 0.007 - mae: 0.116 - mean_q: 0.150 - mean_eps: 0.911 - ale.lives: 2.372\n",
            "\n",
            "Interval 100 (99000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.007 - mae: 0.115 - mean_q: 0.148 - mean_eps: 0.910 - ale.lives: 1.702\n",
            "\n",
            "Interval 101 (100000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 7.500 [7.000, 8.000] - loss: 0.007 - mae: 0.141 - mean_q: 0.183 - mean_eps: 0.910 - ale.lives: 2.008\n",
            "\n",
            "Interval 102 (101000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.007 - mae: 0.144 - mean_q: 0.186 - mean_eps: 0.909 - ale.lives: 2.475\n",
            "\n",
            "Interval 103 (102000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.143 - mean_q: 0.185 - mean_eps: 0.908 - ale.lives: 2.003\n",
            "\n",
            "Interval 104 (103000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.006 - mae: 0.140 - mean_q: 0.178 - mean_eps: 0.907 - ale.lives: 2.174\n",
            "\n",
            "Interval 105 (104000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.006 - mae: 0.141 - mean_q: 0.181 - mean_eps: 0.906 - ale.lives: 2.244\n",
            "\n",
            "Interval 106 (105000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 5.500 [5.000, 6.000] - loss: 0.007 - mae: 0.145 - mean_q: 0.185 - mean_eps: 0.905 - ale.lives: 2.087\n",
            "\n",
            "Interval 107 (106000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 8.000 [5.000, 11.000] - loss: 0.006 - mae: 0.143 - mean_q: 0.184 - mean_eps: 0.904 - ale.lives: 2.019\n",
            "\n",
            "Interval 108 (107000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 5.000 [4.000, 6.000] - loss: 0.007 - mae: 0.144 - mean_q: 0.184 - mean_eps: 0.903 - ale.lives: 2.112\n",
            "\n",
            "Interval 109 (108000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 8.500 [4.000, 13.000] - loss: 0.006 - mae: 0.141 - mean_q: 0.182 - mean_eps: 0.902 - ale.lives: 2.123\n",
            "\n",
            "Interval 110 (109000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.005 - mae: 0.141 - mean_q: 0.180 - mean_eps: 0.901 - ale.lives: 2.083\n",
            "\n",
            "Interval 111 (110000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.007 - mae: 0.170 - mean_q: 0.216 - mean_eps: 0.901 - ale.lives: 1.586\n",
            "\n",
            "Interval 112 (111000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.006 - mae: 0.166 - mean_q: 0.211 - mean_eps: 0.900 - ale.lives: 1.968\n",
            "\n",
            "Interval 113 (112000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 4.500 [4.000, 5.000] - loss: 0.007 - mae: 0.171 - mean_q: 0.217 - mean_eps: 0.899 - ale.lives: 2.216\n",
            "\n",
            "Interval 114 (113000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.006 - mae: 0.167 - mean_q: 0.213 - mean_eps: 0.898 - ale.lives: 2.010\n",
            "\n",
            "Interval 115 (114000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.500 [8.000, 9.000] - loss: 0.006 - mae: 0.170 - mean_q: 0.218 - mean_eps: 0.897 - ale.lives: 2.314\n",
            "\n",
            "Interval 116 (115000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.174 - mean_q: 0.222 - mean_eps: 0.896 - ale.lives: 1.906\n",
            "\n",
            "Interval 117 (116000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.007 - mae: 0.170 - mean_q: 0.217 - mean_eps: 0.895 - ale.lives: 1.803\n",
            "\n",
            "Interval 118 (117000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 6.500 [4.000, 9.000] - loss: 0.007 - mae: 0.168 - mean_q: 0.215 - mean_eps: 0.894 - ale.lives: 2.285\n",
            "\n",
            "Interval 119 (118000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 5.000 [4.000, 6.000] - loss: 0.006 - mae: 0.168 - mean_q: 0.214 - mean_eps: 0.893 - ale.lives: 2.299\n",
            "\n",
            "Interval 120 (119000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.006 - mae: 0.169 - mean_q: 0.216 - mean_eps: 0.892 - ale.lives: 2.085\n",
            "\n",
            "Interval 121 (120000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.007 - mae: 0.192 - mean_q: 0.244 - mean_eps: 0.892 - ale.lives: 1.963\n",
            "\n",
            "Interval 122 (121000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.193 - mean_q: 0.245 - mean_eps: 0.891 - ale.lives: 2.092\n",
            "\n",
            "Interval 123 (122000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.192 - mean_q: 0.245 - mean_eps: 0.890 - ale.lives: 2.251\n",
            "\n",
            "Interval 124 (123000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 13.000 [5.000, 21.000] - loss: 0.007 - mae: 0.197 - mean_q: 0.251 - mean_eps: 0.889 - ale.lives: 1.927\n",
            "\n",
            "Interval 125 (124000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - loss: 0.007 - mae: 0.195 - mean_q: 0.249 - mean_eps: 0.888 - ale.lives: 2.422\n",
            "\n",
            "Interval 126 (125000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.006 - mae: 0.191 - mean_q: 0.244 - mean_eps: 0.887 - ale.lives: 1.866\n",
            "\n",
            "Interval 127 (126000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.007 - mae: 0.195 - mean_q: 0.249 - mean_eps: 0.886 - ale.lives: 1.975\n",
            "\n",
            "Interval 128 (127000 steps performed)\n",
            "1000/1000 [==============================] - 17s 16ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.005 - mae: 0.189 - mean_q: 0.241 - mean_eps: 0.885 - ale.lives: 2.073\n",
            "\n",
            "Interval 129 (128000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.006 - mae: 0.193 - mean_q: 0.244 - mean_eps: 0.884 - ale.lives: 2.126\n",
            "\n",
            "Interval 130 (129000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0030\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.006 - mae: 0.191 - mean_q: 0.242 - mean_eps: 0.883 - ale.lives: 1.903\n",
            "\n",
            "Interval 131 (130000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.007 - mae: 0.215 - mean_q: 0.275 - mean_eps: 0.883 - ale.lives: 1.899\n",
            "\n",
            "Interval 132 (131000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 0.214 - mean_q: 0.273 - mean_eps: 0.882 - ale.lives: 2.179\n",
            "\n",
            "Interval 133 (132000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.212 - mean_q: 0.268 - mean_eps: 0.881 - ale.lives: 2.713\n",
            "\n",
            "Interval 134 (133000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.007 - mae: 0.213 - mean_q: 0.270 - mean_eps: 0.880 - ale.lives: 2.008\n",
            "\n",
            "Interval 135 (134000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.006 - mae: 0.213 - mean_q: 0.269 - mean_eps: 0.879 - ale.lives: 1.996\n",
            "\n",
            "Interval 136 (135000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.214 - mean_q: 0.271 - mean_eps: 0.878 - ale.lives: 1.989\n",
            "\n",
            "Interval 137 (136000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 8.500 [8.000, 9.000] - loss: 0.006 - mae: 0.212 - mean_q: 0.268 - mean_eps: 0.877 - ale.lives: 2.120\n",
            "\n",
            "Interval 138 (137000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.007 - mae: 0.212 - mean_q: 0.267 - mean_eps: 0.876 - ale.lives: 1.998\n",
            "\n",
            "Interval 139 (138000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.005 - mae: 0.210 - mean_q: 0.266 - mean_eps: 0.875 - ale.lives: 2.289\n",
            "\n",
            "Interval 140 (139000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 16.000 [5.000, 27.000] - loss: 0.006 - mae: 0.210 - mean_q: 0.265 - mean_eps: 0.874 - ale.lives: 2.252\n",
            "\n",
            "Interval 141 (140000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.243 - mean_q: 0.307 - mean_eps: 0.874 - ale.lives: 2.117\n",
            "\n",
            "Interval 142 (141000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - loss: 0.007 - mae: 0.245 - mean_q: 0.310 - mean_eps: 0.873 - ale.lives: 1.993\n",
            "\n",
            "Interval 143 (142000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.006 - mae: 0.246 - mean_q: 0.310 - mean_eps: 0.872 - ale.lives: 2.027\n",
            "\n",
            "Interval 144 (143000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.007 - mae: 0.247 - mean_q: 0.310 - mean_eps: 0.871 - ale.lives: 1.626\n",
            "\n",
            "Interval 145 (144000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.006 - mae: 0.244 - mean_q: 0.307 - mean_eps: 0.870 - ale.lives: 2.112\n",
            "\n",
            "Interval 146 (145000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "3 episodes - episode_reward: 6.667 [4.000, 8.000] - loss: 0.006 - mae: 0.244 - mean_q: 0.306 - mean_eps: 0.869 - ale.lives: 2.306\n",
            "\n",
            "Interval 147 (146000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.006 - mae: 0.244 - mean_q: 0.305 - mean_eps: 0.868 - ale.lives: 1.917\n",
            "\n",
            "Interval 148 (147000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.006 - mae: 0.243 - mean_q: 0.305 - mean_eps: 0.867 - ale.lives: 1.577\n",
            "\n",
            "Interval 149 (148000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.007 - mae: 0.246 - mean_q: 0.308 - mean_eps: 0.866 - ale.lives: 2.184\n",
            "\n",
            "Interval 150 (149000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.007 - mae: 0.246 - mean_q: 0.309 - mean_eps: 0.865 - ale.lives: 1.934\n",
            "\n",
            "Interval 151 (150000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 8.500 [4.000, 13.000] - loss: 0.007 - mae: 0.272 - mean_q: 0.343 - mean_eps: 0.865 - ale.lives: 1.892\n",
            "\n",
            "Interval 152 (151000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 8.500 [8.000, 9.000] - loss: 0.007 - mae: 0.271 - mean_q: 0.340 - mean_eps: 0.864 - ale.lives: 2.016\n",
            "\n",
            "Interval 153 (152000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.007 - mae: 0.269 - mean_q: 0.339 - mean_eps: 0.863 - ale.lives: 2.239\n",
            "\n",
            "Interval 154 (153000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.007 - mae: 0.267 - mean_q: 0.334 - mean_eps: 0.862 - ale.lives: 1.790\n",
            "\n",
            "Interval 155 (154000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.006 - mae: 0.265 - mean_q: 0.332 - mean_eps: 0.861 - ale.lives: 2.396\n",
            "\n",
            "Interval 156 (155000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 14.000 [12.000, 16.000] - loss: 0.007 - mae: 0.272 - mean_q: 0.342 - mean_eps: 0.860 - ale.lives: 2.274\n",
            "\n",
            "Interval 157 (156000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0190\n",
            "Interval 158 (157000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.500 [10.000, 19.000] - loss: 0.007 - mae: 0.275 - mean_q: 0.346 - mean_eps: 0.858 - ale.lives: 2.011\n",
            "\n",
            "Interval 159 (158000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.006 - mae: 0.271 - mean_q: 0.340 - mean_eps: 0.857 - ale.lives: 2.066\n",
            "\n",
            "Interval 160 (159000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 7.000 [4.000, 10.000] - loss: 0.007 - mae: 0.270 - mean_q: 0.339 - mean_eps: 0.856 - ale.lives: 2.064\n",
            "\n",
            "Interval 161 (160000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.295 - mean_q: 0.370 - mean_eps: 0.856 - ale.lives: 2.269\n",
            "\n",
            "Interval 162 (161000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 0.299 - mean_q: 0.375 - mean_eps: 0.855 - ale.lives: 1.682\n",
            "\n",
            "Interval 163 (162000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 6.500 [5.000, 8.000] - loss: 0.007 - mae: 0.298 - mean_q: 0.374 - mean_eps: 0.854 - ale.lives: 2.286\n",
            "\n",
            "Interval 164 (163000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 7.500 [7.000, 8.000] - loss: 0.007 - mae: 0.297 - mean_q: 0.372 - mean_eps: 0.853 - ale.lives: 2.118\n",
            "\n",
            "Interval 165 (164000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.302 - mean_q: 0.377 - mean_eps: 0.852 - ale.lives: 2.134\n",
            "\n",
            "Interval 166 (165000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.009 - mae: 0.300 - mean_q: 0.375 - mean_eps: 0.851 - ale.lives: 2.084\n",
            "\n",
            "Interval 167 (166000 steps performed)\n",
            "1000/1000 [==============================] - 15s 15ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - loss: 0.008 - mae: 0.299 - mean_q: 0.375 - mean_eps: 0.850 - ale.lives: 2.285\n",
            "\n",
            "Interval 168 (167000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.007 - mae: 0.299 - mean_q: 0.375 - mean_eps: 0.849 - ale.lives: 2.271\n",
            "\n",
            "Interval 169 (168000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.007 - mae: 0.300 - mean_q: 0.376 - mean_eps: 0.848 - ale.lives: 2.096\n",
            "\n",
            "Interval 170 (169000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.006 - mae: 0.293 - mean_q: 0.365 - mean_eps: 0.847 - ale.lives: 2.065\n",
            "\n",
            "Interval 171 (170000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.336 - mean_q: 0.420 - mean_eps: 0.847 - ale.lives: 1.927\n",
            "\n",
            "Interval 172 (171000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 8.500 [8.000, 9.000] - loss: 0.009 - mae: 0.338 - mean_q: 0.421 - mean_eps: 0.846 - ale.lives: 2.301\n",
            "\n",
            "Interval 173 (172000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.332 - mean_q: 0.414 - mean_eps: 0.845 - ale.lives: 1.769\n",
            "\n",
            "Interval 174 (173000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.009 - mae: 0.334 - mean_q: 0.416 - mean_eps: 0.844 - ale.lives: 1.975\n",
            "\n",
            "Interval 175 (174000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 4.500 [4.000, 5.000] - loss: 0.007 - mae: 0.328 - mean_q: 0.408 - mean_eps: 0.843 - ale.lives: 1.812\n",
            "\n",
            "Interval 176 (175000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.336 - mean_q: 0.418 - mean_eps: 0.842 - ale.lives: 1.888\n",
            "\n",
            "Interval 177 (176000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - loss: 0.008 - mae: 0.334 - mean_q: 0.416 - mean_eps: 0.841 - ale.lives: 1.917\n",
            "\n",
            "Interval 178 (177000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.008 - mae: 0.331 - mean_q: 0.411 - mean_eps: 0.840 - ale.lives: 2.103\n",
            "\n",
            "Interval 179 (178000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.334 - mean_q: 0.414 - mean_eps: 0.839 - ale.lives: 2.109\n",
            "\n",
            "Interval 180 (179000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.332 - mean_q: 0.412 - mean_eps: 0.838 - ale.lives: 2.191\n",
            "\n",
            "Interval 181 (180000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.010 - mae: 0.369 - mean_q: 0.457 - mean_eps: 0.838 - ale.lives: 2.415\n",
            "\n",
            "Interval 182 (181000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 4.500 [4.000, 5.000] - loss: 0.009 - mae: 0.372 - mean_q: 0.461 - mean_eps: 0.837 - ale.lives: 2.080\n",
            "\n",
            "Interval 183 (182000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 12.500 [10.000, 15.000] - loss: 0.008 - mae: 0.369 - mean_q: 0.458 - mean_eps: 0.836 - ale.lives: 2.376\n",
            "\n",
            "Interval 184 (183000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.366 - mean_q: 0.455 - mean_eps: 0.835 - ale.lives: 2.158\n",
            "\n",
            "Interval 185 (184000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.009 - mae: 0.371 - mean_q: 0.460 - mean_eps: 0.834 - ale.lives: 1.952\n",
            "\n",
            "Interval 186 (185000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.009 - mae: 0.371 - mean_q: 0.460 - mean_eps: 0.833 - ale.lives: 2.393\n",
            "\n",
            "Interval 187 (186000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.009 - mae: 0.373 - mean_q: 0.462 - mean_eps: 0.832 - ale.lives: 2.252\n",
            "\n",
            "Interval 188 (187000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.375 - mean_q: 0.466 - mean_eps: 0.831 - ale.lives: 2.237\n",
            "\n",
            "Interval 189 (188000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.009 - mae: 0.370 - mean_q: 0.457 - mean_eps: 0.830 - ale.lives: 2.096\n",
            "\n",
            "Interval 190 (189000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.376 - mean_q: 0.464 - mean_eps: 0.829 - ale.lives: 2.655\n",
            "\n",
            "Interval 191 (190000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.394 - mean_q: 0.488 - mean_eps: 0.829 - ale.lives: 2.079\n",
            "\n",
            "Interval 192 (191000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.399 - mean_q: 0.493 - mean_eps: 0.828 - ale.lives: 2.435\n",
            "\n",
            "Interval 193 (192000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 9.500 [8.000, 11.000] - loss: 0.010 - mae: 0.403 - mean_q: 0.498 - mean_eps: 0.827 - ale.lives: 1.745\n",
            "\n",
            "Interval 194 (193000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0250\n",
            "Interval 195 (194000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 20.000 [13.000, 27.000] - loss: 0.010 - mae: 0.404 - mean_q: 0.501 - mean_eps: 0.825 - ale.lives: 1.804\n",
            "\n",
            "Interval 196 (195000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.401 - mean_q: 0.498 - mean_eps: 0.824 - ale.lives: 2.063\n",
            "\n",
            "Interval 197 (196000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.010 - mae: 0.402 - mean_q: 0.497 - mean_eps: 0.823 - ale.lives: 2.151\n",
            "\n",
            "Interval 198 (197000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.401 - mean_q: 0.496 - mean_eps: 0.822 - ale.lives: 1.896\n",
            "\n",
            "Interval 199 (198000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.399 - mean_q: 0.494 - mean_eps: 0.821 - ale.lives: 2.556\n",
            "\n",
            "Interval 200 (199000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.011 - mae: 0.401 - mean_q: 0.495 - mean_eps: 0.820 - ale.lives: 1.774\n",
            "\n",
            "Interval 201 (200000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.009 - mae: 0.420 - mean_q: 0.521 - mean_eps: 0.820 - ale.lives: 2.369\n",
            "\n",
            "Interval 202 (201000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.008 - mae: 0.425 - mean_q: 0.525 - mean_eps: 0.819 - ale.lives: 1.934\n",
            "\n",
            "Interval 203 (202000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.429 - mean_q: 0.530 - mean_eps: 0.818 - ale.lives: 1.802\n",
            "\n",
            "Interval 204 (203000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.418 - mean_q: 0.518 - mean_eps: 0.817 - ale.lives: 2.116\n",
            "\n",
            "Interval 205 (204000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.428 - mean_q: 0.529 - mean_eps: 0.816 - ale.lives: 2.363\n",
            "\n",
            "Interval 206 (205000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.423 - mean_q: 0.522 - mean_eps: 0.815 - ale.lives: 2.124\n",
            "\n",
            "Interval 207 (206000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 15.500 [14.000, 17.000] - loss: 0.009 - mae: 0.422 - mean_q: 0.521 - mean_eps: 0.814 - ale.lives: 1.842\n",
            "\n",
            "Interval 208 (207000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.430 - mean_q: 0.531 - mean_eps: 0.813 - ale.lives: 2.314\n",
            "\n",
            "Interval 209 (208000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 6.000 [4.000, 8.000] - loss: 0.009 - mae: 0.423 - mean_q: 0.522 - mean_eps: 0.812 - ale.lives: 1.980\n",
            "\n",
            "Interval 210 (209000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 10.500 [10.000, 11.000] - loss: 0.009 - mae: 0.429 - mean_q: 0.531 - mean_eps: 0.811 - ale.lives: 2.082\n",
            "\n",
            "Interval 211 (210000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.453 - mean_q: 0.562 - mean_eps: 0.811 - ale.lives: 2.113\n",
            "\n",
            "Interval 212 (211000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 7.000 [3.000, 11.000] - loss: 0.010 - mae: 0.455 - mean_q: 0.561 - mean_eps: 0.810 - ale.lives: 2.141\n",
            "\n",
            "Interval 213 (212000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.010 - mae: 0.450 - mean_q: 0.555 - mean_eps: 0.809 - ale.lives: 2.150\n",
            "\n",
            "Interval 214 (213000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.500 [4.000, 21.000] - loss: 0.010 - mae: 0.450 - mean_q: 0.557 - mean_eps: 0.808 - ale.lives: 1.609\n",
            "\n",
            "Interval 215 (214000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 7.500 [7.000, 8.000] - loss: 0.008 - mae: 0.454 - mean_q: 0.563 - mean_eps: 0.807 - ale.lives: 2.422\n",
            "\n",
            "Interval 216 (215000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 5.000 [2.000, 8.000] - loss: 0.009 - mae: 0.462 - mean_q: 0.571 - mean_eps: 0.806 - ale.lives: 1.987\n",
            "\n",
            "Interval 217 (216000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.446 - mean_q: 0.551 - mean_eps: 0.805 - ale.lives: 2.345\n",
            "\n",
            "Interval 218 (217000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.449 - mean_q: 0.554 - mean_eps: 0.804 - ale.lives: 2.440\n",
            "\n",
            "Interval 219 (218000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 6.500 [5.000, 8.000] - loss: 0.008 - mae: 0.450 - mean_q: 0.556 - mean_eps: 0.803 - ale.lives: 2.142\n",
            "\n",
            "Interval 220 (219000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0080\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.008 - mae: 0.452 - mean_q: 0.557 - mean_eps: 0.802 - ale.lives: 2.063\n",
            "\n",
            "Interval 221 (220000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.010 - mae: 0.480 - mean_q: 0.593 - mean_eps: 0.802 - ale.lives: 1.653\n",
            "\n",
            "Interval 222 (221000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 10.500 [6.000, 15.000] - loss: 0.010 - mae: 0.488 - mean_q: 0.601 - mean_eps: 0.801 - ale.lives: 2.222\n",
            "\n",
            "Interval 223 (222000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.010 - mae: 0.483 - mean_q: 0.596 - mean_eps: 0.800 - ale.lives: 2.256\n",
            "\n",
            "Interval 224 (223000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.010 - mae: 0.483 - mean_q: 0.596 - mean_eps: 0.799 - ale.lives: 2.305\n",
            "\n",
            "Interval 225 (224000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 10.500 [10.000, 11.000] - loss: 0.010 - mae: 0.485 - mean_q: 0.599 - mean_eps: 0.798 - ale.lives: 2.225\n",
            "\n",
            "Interval 226 (225000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.484 - mean_q: 0.599 - mean_eps: 0.797 - ale.lives: 2.447\n",
            "\n",
            "Interval 227 (226000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - loss: 0.010 - mae: 0.487 - mean_q: 0.601 - mean_eps: 0.796 - ale.lives: 2.272\n",
            "\n",
            "Interval 228 (227000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.483 - mean_q: 0.596 - mean_eps: 0.795 - ale.lives: 2.259\n",
            "\n",
            "Interval 229 (228000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.482 - mean_q: 0.595 - mean_eps: 0.794 - ale.lives: 1.862\n",
            "\n",
            "Interval 230 (229000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.010 - mae: 0.485 - mean_q: 0.599 - mean_eps: 0.793 - ale.lives: 2.182\n",
            "\n",
            "Interval 231 (230000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.522 - mean_q: 0.647 - mean_eps: 0.793 - ale.lives: 2.512\n",
            "\n",
            "Interval 232 (231000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.010 - mae: 0.522 - mean_q: 0.648 - mean_eps: 0.792 - ale.lives: 1.700\n",
            "\n",
            "Interval 233 (232000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.518 - mean_q: 0.642 - mean_eps: 0.791 - ale.lives: 2.101\n",
            "\n",
            "Interval 234 (233000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 0.009 - mae: 0.523 - mean_q: 0.648 - mean_eps: 0.790 - ale.lives: 1.991\n",
            "\n",
            "Interval 235 (234000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.011 - mae: 0.531 - mean_q: 0.657 - mean_eps: 0.789 - ale.lives: 2.306\n",
            "\n",
            "Interval 236 (235000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0060\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.522 - mean_q: 0.647 - mean_eps: 0.788 - ale.lives: 1.522\n",
            "\n",
            "Interval 237 (236000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0290\n",
            "1 episodes - episode_reward: 31.000 [31.000, 31.000] - loss: 0.010 - mae: 0.522 - mean_q: 0.645 - mean_eps: 0.787 - ale.lives: 1.576\n",
            "\n",
            "Interval 238 (237000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.520 - mean_q: 0.644 - mean_eps: 0.786 - ale.lives: 2.389\n",
            "\n",
            "Interval 239 (238000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.009 - mae: 0.518 - mean_q: 0.639 - mean_eps: 0.785 - ale.lives: 2.112\n",
            "\n",
            "Interval 240 (239000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.010 - mae: 0.529 - mean_q: 0.653 - mean_eps: 0.784 - ale.lives: 2.224\n",
            "\n",
            "Interval 241 (240000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.011 - mae: 0.557 - mean_q: 0.689 - mean_eps: 0.784 - ale.lives: 2.258\n",
            "\n",
            "Interval 242 (241000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.010 - mae: 0.553 - mean_q: 0.683 - mean_eps: 0.783 - ale.lives: 2.322\n",
            "\n",
            "Interval 243 (242000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.011 - mae: 0.559 - mean_q: 0.692 - mean_eps: 0.782 - ale.lives: 1.657\n",
            "\n",
            "Interval 244 (243000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [6.000, 16.000] - loss: 0.010 - mae: 0.551 - mean_q: 0.681 - mean_eps: 0.781 - ale.lives: 2.265\n",
            "\n",
            "Interval 245 (244000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.011 - mae: 0.552 - mean_q: 0.683 - mean_eps: 0.780 - ale.lives: 2.174\n",
            "\n",
            "Interval 246 (245000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.010 - mae: 0.554 - mean_q: 0.684 - mean_eps: 0.779 - ale.lives: 2.009\n",
            "\n",
            "Interval 247 (246000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.010 - mae: 0.552 - mean_q: 0.680 - mean_eps: 0.778 - ale.lives: 2.309\n",
            "\n",
            "Interval 248 (247000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.553 - mean_q: 0.682 - mean_eps: 0.777 - ale.lives: 1.943\n",
            "\n",
            "Interval 249 (248000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.000 [9.000, 19.000] - loss: 0.010 - mae: 0.569 - mean_q: 0.703 - mean_eps: 0.776 - ale.lives: 2.029\n",
            "\n",
            "Interval 250 (249000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.010 - mae: 0.561 - mean_q: 0.694 - mean_eps: 0.775 - ale.lives: 2.107\n",
            "\n",
            "Interval 251 (250000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.010 - mae: 0.580 - mean_q: 0.716 - mean_eps: 0.775 - ale.lives: 2.111\n",
            "\n",
            "Interval 252 (251000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "Interval 253 (252000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 13.500 [3.000, 24.000] - loss: 0.010 - mae: 0.585 - mean_q: 0.724 - mean_eps: 0.773 - ale.lives: 2.154\n",
            "\n",
            "Interval 254 (253000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.011 - mae: 0.577 - mean_q: 0.713 - mean_eps: 0.772 - ale.lives: 2.017\n",
            "\n",
            "Interval 255 (254000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.010 - mae: 0.575 - mean_q: 0.709 - mean_eps: 0.771 - ale.lives: 2.558\n",
            "\n",
            "Interval 256 (255000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 13.000 [8.000, 18.000] - loss: 0.010 - mae: 0.579 - mean_q: 0.715 - mean_eps: 0.770 - ale.lives: 2.126\n",
            "\n",
            "Interval 257 (256000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.010 - mae: 0.583 - mean_q: 0.717 - mean_eps: 0.769 - ale.lives: 2.210\n",
            "\n",
            "Interval 258 (257000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.010 - mae: 0.580 - mean_q: 0.714 - mean_eps: 0.768 - ale.lives: 2.142\n",
            "\n",
            "Interval 259 (258000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.011 - mae: 0.584 - mean_q: 0.721 - mean_eps: 0.767 - ale.lives: 1.842\n",
            "\n",
            "Interval 260 (259000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 14.000 [8.000, 20.000] - loss: 0.010 - mae: 0.572 - mean_q: 0.706 - mean_eps: 0.766 - ale.lives: 2.053\n",
            "\n",
            "Interval 261 (260000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.011 - mae: 0.632 - mean_q: 0.781 - mean_eps: 0.766 - ale.lives: 2.291\n",
            "\n",
            "Interval 262 (261000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.012 - mae: 0.621 - mean_q: 0.766 - mean_eps: 0.765 - ale.lives: 1.853\n",
            "\n",
            "Interval 263 (262000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 8.000 [5.000, 11.000] - loss: 0.011 - mae: 0.629 - mean_q: 0.777 - mean_eps: 0.764 - ale.lives: 2.019\n",
            "\n",
            "Interval 264 (263000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.624 - mean_q: 0.769 - mean_eps: 0.763 - ale.lives: 1.585\n",
            "\n",
            "Interval 265 (264000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.012 - mae: 0.620 - mean_q: 0.764 - mean_eps: 0.762 - ale.lives: 2.169\n",
            "\n",
            "Interval 266 (265000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 15.000 [12.000, 18.000] - loss: 0.011 - mae: 0.629 - mean_q: 0.775 - mean_eps: 0.761 - ale.lives: 2.307\n",
            "\n",
            "Interval 267 (266000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.011 - mae: 0.625 - mean_q: 0.771 - mean_eps: 0.760 - ale.lives: 2.228\n",
            "\n",
            "Interval 268 (267000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.010 - mae: 0.624 - mean_q: 0.768 - mean_eps: 0.759 - ale.lives: 1.953\n",
            "\n",
            "Interval 269 (268000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.000 [9.000, 15.000] - loss: 0.010 - mae: 0.624 - mean_q: 0.770 - mean_eps: 0.758 - ale.lives: 2.403\n",
            "\n",
            "Interval 270 (269000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.011 - mae: 0.623 - mean_q: 0.768 - mean_eps: 0.757 - ale.lives: 2.335\n",
            "\n",
            "Interval 271 (270000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.010 - mae: 0.652 - mean_q: 0.804 - mean_eps: 0.757 - ale.lives: 1.848\n",
            "\n",
            "Interval 272 (271000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 11.500 [6.000, 17.000] - loss: 0.010 - mae: 0.660 - mean_q: 0.814 - mean_eps: 0.756 - ale.lives: 1.680\n",
            "\n",
            "Interval 273 (272000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.011 - mae: 0.669 - mean_q: 0.826 - mean_eps: 0.755 - ale.lives: 2.383\n",
            "\n",
            "Interval 274 (273000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.011 - mae: 0.656 - mean_q: 0.808 - mean_eps: 0.754 - ale.lives: 2.036\n",
            "\n",
            "Interval 275 (274000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0090\n",
            "Interval 276 (275000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.011 - mae: 0.667 - mean_q: 0.820 - mean_eps: 0.752 - ale.lives: 2.044\n",
            "\n",
            "Interval 277 (276000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.011 - mae: 0.668 - mean_q: 0.824 - mean_eps: 0.751 - ale.lives: 2.277\n",
            "\n",
            "Interval 278 (277000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.011 - mae: 0.667 - mean_q: 0.824 - mean_eps: 0.750 - ale.lives: 2.299\n",
            "\n",
            "Interval 279 (278000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.011 - mae: 0.665 - mean_q: 0.821 - mean_eps: 0.749 - ale.lives: 1.633\n",
            "\n",
            "Interval 280 (279000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.010 - mae: 0.662 - mean_q: 0.814 - mean_eps: 0.748 - ale.lives: 2.393\n",
            "\n",
            "Interval 281 (280000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 18.000 [12.000, 24.000] - loss: 0.011 - mae: 0.710 - mean_q: 0.874 - mean_eps: 0.748 - ale.lives: 2.041\n",
            "\n",
            "Interval 282 (281000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 0.011 - mae: 0.701 - mean_q: 0.863 - mean_eps: 0.747 - ale.lives: 1.883\n",
            "\n",
            "Interval 283 (282000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.012 - mae: 0.719 - mean_q: 0.883 - mean_eps: 0.746 - ale.lives: 1.925\n",
            "\n",
            "Interval 284 (283000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.000 [9.000, 11.000] - loss: 0.011 - mae: 0.716 - mean_q: 0.881 - mean_eps: 0.745 - ale.lives: 2.214\n",
            "\n",
            "Interval 285 (284000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.011 - mae: 0.714 - mean_q: 0.877 - mean_eps: 0.744 - ale.lives: 2.278\n",
            "\n",
            "Interval 286 (285000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.011 - mae: 0.714 - mean_q: 0.878 - mean_eps: 0.743 - ale.lives: 2.188\n",
            "\n",
            "Interval 287 (286000 steps performed)\n",
            "1000/1000 [==============================] - 21s 20ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.012 - mae: 0.713 - mean_q: 0.877 - mean_eps: 0.742 - ale.lives: 1.916\n",
            "\n",
            "Interval 288 (287000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.500 [12.000, 15.000] - loss: 0.012 - mae: 0.711 - mean_q: 0.872 - mean_eps: 0.741 - ale.lives: 2.014\n",
            "\n",
            "Interval 289 (288000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.013 - mae: 0.715 - mean_q: 0.878 - mean_eps: 0.740 - ale.lives: 2.063\n",
            "\n",
            "Interval 290 (289000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.012 - mae: 0.717 - mean_q: 0.880 - mean_eps: 0.739 - ale.lives: 1.672\n",
            "\n",
            "Interval 291 (290000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 9.500 [9.000, 10.000] - loss: 0.011 - mae: 0.747 - mean_q: 0.916 - mean_eps: 0.739 - ale.lives: 1.984\n",
            "\n",
            "Interval 292 (291000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 11.000 [5.000, 17.000] - loss: 0.011 - mae: 0.757 - mean_q: 0.932 - mean_eps: 0.738 - ale.lives: 1.946\n",
            "\n",
            "Interval 293 (292000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.012 - mae: 0.747 - mean_q: 0.915 - mean_eps: 0.737 - ale.lives: 2.112\n",
            "\n",
            "Interval 294 (293000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.011 - mae: 0.758 - mean_q: 0.930 - mean_eps: 0.736 - ale.lives: 2.293\n",
            "\n",
            "Interval 295 (294000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 0.770 - mean_q: 0.945 - mean_eps: 0.735 - ale.lives: 2.318\n",
            "\n",
            "Interval 296 (295000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.012 - mae: 0.749 - mean_q: 0.919 - mean_eps: 0.734 - ale.lives: 2.441\n",
            "\n",
            "Interval 297 (296000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.012 - mae: 0.754 - mean_q: 0.927 - mean_eps: 0.733 - ale.lives: 2.199\n",
            "\n",
            "Interval 298 (297000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.013 - mae: 0.750 - mean_q: 0.920 - mean_eps: 0.732 - ale.lives: 2.403\n",
            "\n",
            "Interval 299 (298000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.012 - mae: 0.755 - mean_q: 0.926 - mean_eps: 0.731 - ale.lives: 2.204\n",
            "\n",
            "Interval 300 (299000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.012 - mae: 0.752 - mean_q: 0.923 - mean_eps: 0.730 - ale.lives: 2.141\n",
            "\n",
            "Interval 301 (300000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.012 - mae: 0.792 - mean_q: 0.971 - mean_eps: 0.730 - ale.lives: 1.756\n",
            "\n",
            "Interval 302 (301000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.012 - mae: 0.782 - mean_q: 0.961 - mean_eps: 0.729 - ale.lives: 2.030\n",
            "\n",
            "Interval 303 (302000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - loss: 0.013 - mae: 0.790 - mean_q: 0.969 - mean_eps: 0.728 - ale.lives: 2.267\n",
            "\n",
            "Interval 304 (303000 steps performed)\n",
            "1000/1000 [==============================] - 24s 23ms/step - reward: 0.0200\n",
            "3 episodes - episode_reward: 7.000 [6.000, 8.000] - loss: 0.012 - mae: 0.792 - mean_q: 0.973 - mean_eps: 0.727 - ale.lives: 2.348\n",
            "\n",
            "Interval 305 (304000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.013 - mae: 0.792 - mean_q: 0.972 - mean_eps: 0.726 - ale.lives: 2.075\n",
            "\n",
            "Interval 306 (305000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.012 - mae: 0.779 - mean_q: 0.957 - mean_eps: 0.725 - ale.lives: 1.991\n",
            "\n",
            "Interval 307 (306000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.013 - mae: 0.794 - mean_q: 0.974 - mean_eps: 0.724 - ale.lives: 1.922\n",
            "\n",
            "Interval 308 (307000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 15.500 [12.000, 19.000] - loss: 0.013 - mae: 0.795 - mean_q: 0.975 - mean_eps: 0.723 - ale.lives: 1.553\n",
            "\n",
            "Interval 309 (308000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.012 - mae: 0.797 - mean_q: 0.975 - mean_eps: 0.722 - ale.lives: 2.280\n",
            "\n",
            "Interval 310 (309000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.016 - mae: 0.799 - mean_q: 0.983 - mean_eps: 0.721 - ale.lives: 1.803\n",
            "\n",
            "Interval 311 (310000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 0.012 - mae: 0.818 - mean_q: 1.005 - mean_eps: 0.721 - ale.lives: 2.010\n",
            "\n",
            "Interval 312 (311000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.012 - mae: 0.820 - mean_q: 1.006 - mean_eps: 0.720 - ale.lives: 2.373\n",
            "\n",
            "Interval 313 (312000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.014 - mae: 0.828 - mean_q: 1.014 - mean_eps: 0.719 - ale.lives: 2.038\n",
            "\n",
            "Interval 314 (313000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.014 - mae: 0.819 - mean_q: 1.003 - mean_eps: 0.718 - ale.lives: 2.415\n",
            "\n",
            "Interval 315 (314000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.013 - mae: 0.817 - mean_q: 1.001 - mean_eps: 0.717 - ale.lives: 2.344\n",
            "\n",
            "Interval 316 (315000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 9.000 [5.000, 13.000] - loss: 0.013 - mae: 0.821 - mean_q: 1.007 - mean_eps: 0.716 - ale.lives: 1.795\n",
            "\n",
            "Interval 317 (316000 steps performed)\n",
            "1000/1000 [==============================] - 21s 20ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.013 - mae: 0.813 - mean_q: 0.997 - mean_eps: 0.715 - ale.lives: 2.033\n",
            "\n",
            "Interval 318 (317000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.013 - mae: 0.825 - mean_q: 1.010 - mean_eps: 0.714 - ale.lives: 2.325\n",
            "\n",
            "Interval 319 (318000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 15.000 [10.000, 20.000] - loss: 0.012 - mae: 0.822 - mean_q: 1.008 - mean_eps: 0.713 - ale.lives: 1.721\n",
            "\n",
            "Interval 320 (319000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.500 [8.000, 9.000] - loss: 0.013 - mae: 0.808 - mean_q: 0.990 - mean_eps: 0.712 - ale.lives: 2.053\n",
            "\n",
            "Interval 321 (320000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.012 - mae: 0.838 - mean_q: 1.031 - mean_eps: 0.712 - ale.lives: 2.463\n",
            "\n",
            "Interval 322 (321000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.013 - mae: 0.845 - mean_q: 1.036 - mean_eps: 0.711 - ale.lives: 2.113\n",
            "\n",
            "Interval 323 (322000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.013 - mae: 0.843 - mean_q: 1.033 - mean_eps: 0.710 - ale.lives: 2.155\n",
            "\n",
            "Interval 324 (323000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.013 - mae: 0.844 - mean_q: 1.034 - mean_eps: 0.709 - ale.lives: 2.228\n",
            "\n",
            "Interval 325 (324000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.013 - mae: 0.840 - mean_q: 1.030 - mean_eps: 0.708 - ale.lives: 1.469\n",
            "\n",
            "Interval 326 (325000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.014 - mae: 0.845 - mean_q: 1.033 - mean_eps: 0.707 - ale.lives: 2.379\n",
            "\n",
            "Interval 327 (326000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 17.000 [13.000, 21.000] - loss: 0.011 - mae: 0.840 - mean_q: 1.029 - mean_eps: 0.706 - ale.lives: 2.235\n",
            "\n",
            "Interval 328 (327000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.012 - mae: 0.841 - mean_q: 1.031 - mean_eps: 0.705 - ale.lives: 2.151\n",
            "\n",
            "Interval 329 (328000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.011 - mae: 0.842 - mean_q: 1.029 - mean_eps: 0.704 - ale.lives: 1.771\n",
            "\n",
            "Interval 330 (329000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 0.840 - mean_q: 1.031 - mean_eps: 0.703 - ale.lives: 1.921\n",
            "\n",
            "Interval 331 (330000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.011 - mae: 0.889 - mean_q: 1.092 - mean_eps: 0.703 - ale.lives: 1.929\n",
            "\n",
            "Interval 332 (331000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.014 - mae: 0.889 - mean_q: 1.092 - mean_eps: 0.702 - ale.lives: 2.158\n",
            "\n",
            "Interval 333 (332000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 14.000 [12.000, 16.000] - loss: 0.012 - mae: 0.881 - mean_q: 1.081 - mean_eps: 0.701 - ale.lives: 1.745\n",
            "\n",
            "Interval 334 (333000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.012 - mae: 0.900 - mean_q: 1.104 - mean_eps: 0.700 - ale.lives: 2.150\n",
            "\n",
            "Interval 335 (334000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.013 - mae: 0.905 - mean_q: 1.107 - mean_eps: 0.699 - ale.lives: 2.022\n",
            "\n",
            "Interval 336 (335000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 12.500 [11.000, 14.000] - loss: 0.014 - mae: 0.897 - mean_q: 1.097 - mean_eps: 0.698 - ale.lives: 2.186\n",
            "\n",
            "Interval 337 (336000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 11.000 [8.000, 14.000] - loss: 0.013 - mae: 0.900 - mean_q: 1.104 - mean_eps: 0.697 - ale.lives: 2.140\n",
            "\n",
            "Interval 338 (337000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.013 - mae: 0.900 - mean_q: 1.101 - mean_eps: 0.696 - ale.lives: 2.328\n",
            "\n",
            "Interval 339 (338000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 0.896 - mean_q: 1.095 - mean_eps: 0.695 - ale.lives: 2.079\n",
            "\n",
            "Interval 340 (339000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 0.887 - mean_q: 1.085 - mean_eps: 0.694 - ale.lives: 1.854\n",
            "\n",
            "Interval 341 (340000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 8.500 [7.000, 10.000] - loss: 0.012 - mae: 0.903 - mean_q: 1.107 - mean_eps: 0.694 - ale.lives: 2.505\n",
            "\n",
            "Interval 342 (341000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.013 - mae: 0.908 - mean_q: 1.112 - mean_eps: 0.693 - ale.lives: 2.111\n",
            "\n",
            "Interval 343 (342000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.012 - mae: 0.898 - mean_q: 1.099 - mean_eps: 0.692 - ale.lives: 2.179\n",
            "\n",
            "Interval 344 (343000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.012 - mae: 0.890 - mean_q: 1.090 - mean_eps: 0.691 - ale.lives: 2.078\n",
            "\n",
            "Interval 345 (344000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.011 - mae: 0.885 - mean_q: 1.082 - mean_eps: 0.690 - ale.lives: 1.946\n",
            "\n",
            "Interval 346 (345000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.013 - mae: 0.901 - mean_q: 1.104 - mean_eps: 0.689 - ale.lives: 2.156\n",
            "\n",
            "Interval 347 (346000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "3 episodes - episode_reward: 8.667 [5.000, 14.000] - loss: 0.012 - mae: 0.892 - mean_q: 1.092 - mean_eps: 0.688 - ale.lives: 2.032\n",
            "\n",
            "Interval 348 (347000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.012 - mae: 0.897 - mean_q: 1.099 - mean_eps: 0.687 - ale.lives: 2.594\n",
            "\n",
            "Interval 349 (348000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.012 - mae: 0.898 - mean_q: 1.100 - mean_eps: 0.686 - ale.lives: 2.278\n",
            "\n",
            "Interval 350 (349000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.014 - mae: 0.910 - mean_q: 1.114 - mean_eps: 0.685 - ale.lives: 2.077\n",
            "\n",
            "Interval 351 (350000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.013 - mae: 0.926 - mean_q: 1.134 - mean_eps: 0.685 - ale.lives: 2.088\n",
            "\n",
            "Interval 352 (351000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "Interval 353 (352000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 16.500 [8.000, 25.000] - loss: 0.012 - mae: 0.913 - mean_q: 1.116 - mean_eps: 0.683 - ale.lives: 2.500\n",
            "\n",
            "Interval 354 (353000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.012 - mae: 0.915 - mean_q: 1.120 - mean_eps: 0.682 - ale.lives: 2.280\n",
            "\n",
            "Interval 355 (354000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.014 - mae: 0.931 - mean_q: 1.142 - mean_eps: 0.681 - ale.lives: 2.137\n",
            "\n",
            "Interval 356 (355000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 8.000 [7.000, 9.000] - loss: 0.012 - mae: 0.930 - mean_q: 1.137 - mean_eps: 0.680 - ale.lives: 2.258\n",
            "\n",
            "Interval 357 (356000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.012 - mae: 0.914 - mean_q: 1.118 - mean_eps: 0.679 - ale.lives: 2.280\n",
            "\n",
            "Interval 358 (357000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.013 - mae: 0.918 - mean_q: 1.122 - mean_eps: 0.678 - ale.lives: 2.135\n",
            "\n",
            "Interval 359 (358000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.012 - mae: 0.932 - mean_q: 1.138 - mean_eps: 0.677 - ale.lives: 2.135\n",
            "\n",
            "Interval 360 (359000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.013 - mae: 0.921 - mean_q: 1.126 - mean_eps: 0.676 - ale.lives: 2.258\n",
            "\n",
            "Interval 361 (360000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.012 - mae: 0.959 - mean_q: 1.174 - mean_eps: 0.676 - ale.lives: 1.623\n",
            "\n",
            "Interval 362 (361000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.012 - mae: 0.961 - mean_q: 1.177 - mean_eps: 0.675 - ale.lives: 2.351\n",
            "\n",
            "Interval 363 (362000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.012 - mae: 0.955 - mean_q: 1.167 - mean_eps: 0.674 - ale.lives: 1.972\n",
            "\n",
            "Interval 364 (363000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.018 - mae: 0.986 - mean_q: 1.208 - mean_eps: 0.673 - ale.lives: 2.486\n",
            "\n",
            "Interval 365 (364000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.015 - mae: 0.972 - mean_q: 1.191 - mean_eps: 0.672 - ale.lives: 1.706\n",
            "\n",
            "Interval 366 (365000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 15.000 [12.000, 18.000] - loss: 0.015 - mae: 0.972 - mean_q: 1.190 - mean_eps: 0.671 - ale.lives: 2.292\n",
            "\n",
            "Interval 367 (366000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.013 - mae: 0.977 - mean_q: 1.195 - mean_eps: 0.670 - ale.lives: 2.203\n",
            "\n",
            "Interval 368 (367000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.013 - mae: 0.959 - mean_q: 1.176 - mean_eps: 0.669 - ale.lives: 2.291\n",
            "\n",
            "Interval 369 (368000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.012 - mae: 0.966 - mean_q: 1.181 - mean_eps: 0.668 - ale.lives: 1.985\n",
            "\n",
            "Interval 370 (369000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 16.500 [11.000, 22.000] - loss: 0.013 - mae: 0.965 - mean_q: 1.180 - mean_eps: 0.667 - ale.lives: 2.302\n",
            "\n",
            "Interval 371 (370000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 30.000 [30.000, 30.000] - loss: 0.012 - mae: 0.985 - mean_q: 1.204 - mean_eps: 0.667 - ale.lives: 2.121\n",
            "\n",
            "Interval 372 (371000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "Interval 373 (372000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 20.000 [16.000, 24.000] - loss: 0.012 - mae: 0.993 - mean_q: 1.215 - mean_eps: 0.665 - ale.lives: 1.944\n",
            "\n",
            "Interval 374 (373000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.014 - mae: 1.004 - mean_q: 1.226 - mean_eps: 0.664 - ale.lives: 2.224\n",
            "\n",
            "Interval 375 (374000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.013 - mae: 1.004 - mean_q: 1.228 - mean_eps: 0.663 - ale.lives: 2.197\n",
            "\n",
            "Interval 376 (375000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.013 - mae: 0.991 - mean_q: 1.211 - mean_eps: 0.662 - ale.lives: 1.653\n",
            "\n",
            "Interval 377 (376000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 0.984 - mean_q: 1.200 - mean_eps: 0.661 - ale.lives: 2.273\n",
            "\n",
            "Interval 378 (377000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.015 - mae: 1.001 - mean_q: 1.221 - mean_eps: 0.660 - ale.lives: 2.377\n",
            "\n",
            "Interval 379 (378000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 13.000 [9.000, 17.000] - loss: 0.015 - mae: 0.995 - mean_q: 1.215 - mean_eps: 0.659 - ale.lives: 2.227\n",
            "\n",
            "Interval 380 (379000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.014 - mae: 1.003 - mean_q: 1.228 - mean_eps: 0.658 - ale.lives: 2.023\n",
            "\n",
            "Interval 381 (380000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.014 - mae: 1.014 - mean_q: 1.238 - mean_eps: 0.658 - ale.lives: 2.176\n",
            "\n",
            "Interval 382 (381000 steps performed)\n",
            "1000/1000 [==============================] - 24s 23ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.015 - mae: 1.034 - mean_q: 1.264 - mean_eps: 0.657 - ale.lives: 1.865\n",
            "\n",
            "Interval 383 (382000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.013 - mae: 1.017 - mean_q: 1.243 - mean_eps: 0.656 - ale.lives: 2.446\n",
            "\n",
            "Interval 384 (383000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.015 - mae: 1.025 - mean_q: 1.250 - mean_eps: 0.655 - ale.lives: 2.292\n",
            "\n",
            "Interval 385 (384000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 1.018 - mean_q: 1.243 - mean_eps: 0.654 - ale.lives: 2.512\n",
            "\n",
            "Interval 386 (385000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.012 - mae: 1.030 - mean_q: 1.259 - mean_eps: 0.653 - ale.lives: 2.396\n",
            "\n",
            "Interval 387 (386000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.500 [9.000, 14.000] - loss: 0.014 - mae: 1.022 - mean_q: 1.248 - mean_eps: 0.652 - ale.lives: 1.933\n",
            "\n",
            "Interval 388 (387000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 7.000 [4.000, 10.000] - loss: 0.012 - mae: 1.024 - mean_q: 1.249 - mean_eps: 0.651 - ale.lives: 1.797\n",
            "\n",
            "Interval 389 (388000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.016 - mae: 1.021 - mean_q: 1.246 - mean_eps: 0.650 - ale.lives: 2.257\n",
            "\n",
            "Interval 390 (389000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.012 - mae: 1.036 - mean_q: 1.264 - mean_eps: 0.649 - ale.lives: 2.096\n",
            "\n",
            "Interval 391 (390000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.012 - mae: 1.064 - mean_q: 1.300 - mean_eps: 0.649 - ale.lives: 1.859\n",
            "\n",
            "Interval 392 (391000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0320\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.015 - mae: 1.070 - mean_q: 1.306 - mean_eps: 0.648 - ale.lives: 2.737\n",
            "\n",
            "Interval 393 (392000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 35.000 [35.000, 35.000] - loss: 0.015 - mae: 1.054 - mean_q: 1.289 - mean_eps: 0.647 - ale.lives: 2.216\n",
            "\n",
            "Interval 394 (393000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 9.000 [5.000, 13.000] - loss: 0.015 - mae: 1.066 - mean_q: 1.303 - mean_eps: 0.646 - ale.lives: 2.267\n",
            "\n",
            "Interval 395 (394000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.500 [8.000, 11.000] - loss: 0.013 - mae: 1.070 - mean_q: 1.306 - mean_eps: 0.645 - ale.lives: 1.834\n",
            "\n",
            "Interval 396 (395000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.012 - mae: 1.054 - mean_q: 1.286 - mean_eps: 0.644 - ale.lives: 2.100\n",
            "\n",
            "Interval 397 (396000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.014 - mae: 1.058 - mean_q: 1.293 - mean_eps: 0.643 - ale.lives: 1.934\n",
            "\n",
            "Interval 398 (397000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 14.000 [12.000, 16.000] - loss: 0.013 - mae: 1.067 - mean_q: 1.300 - mean_eps: 0.642 - ale.lives: 2.319\n",
            "\n",
            "Interval 399 (398000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.014 - mae: 1.048 - mean_q: 1.278 - mean_eps: 0.641 - ale.lives: 2.083\n",
            "\n",
            "Interval 400 (399000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 9.500 [8.000, 11.000] - loss: 0.015 - mae: 1.068 - mean_q: 1.302 - mean_eps: 0.640 - ale.lives: 2.068\n",
            "\n",
            "Interval 401 (400000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.013 - mae: 1.115 - mean_q: 1.360 - mean_eps: 0.640 - ale.lives: 2.244\n",
            "\n",
            "Interval 402 (401000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.016 - mae: 1.137 - mean_q: 1.387 - mean_eps: 0.639 - ale.lives: 2.033\n",
            "\n",
            "Interval 403 (402000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.013 - mae: 1.113 - mean_q: 1.360 - mean_eps: 0.638 - ale.lives: 2.508\n",
            "\n",
            "Interval 404 (403000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 11.500 [9.000, 14.000] - loss: 0.014 - mae: 1.133 - mean_q: 1.384 - mean_eps: 0.637 - ale.lives: 1.872\n",
            "\n",
            "Interval 405 (404000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.015 - mae: 1.116 - mean_q: 1.361 - mean_eps: 0.636 - ale.lives: 2.172\n",
            "\n",
            "Interval 406 (405000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 1.110 - mean_q: 1.352 - mean_eps: 0.635 - ale.lives: 2.205\n",
            "\n",
            "Interval 407 (406000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.016 - mae: 1.116 - mean_q: 1.361 - mean_eps: 0.634 - ale.lives: 1.989\n",
            "\n",
            "Interval 408 (407000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.013 - mae: 1.114 - mean_q: 1.357 - mean_eps: 0.633 - ale.lives: 2.469\n",
            "\n",
            "Interval 409 (408000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.015 - mae: 1.118 - mean_q: 1.361 - mean_eps: 0.632 - ale.lives: 2.161\n",
            "\n",
            "Interval 410 (409000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.015 - mae: 1.117 - mean_q: 1.362 - mean_eps: 0.631 - ale.lives: 1.923\n",
            "\n",
            "Interval 411 (410000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.012 - mae: 1.122 - mean_q: 1.370 - mean_eps: 0.631 - ale.lives: 2.194\n",
            "\n",
            "Interval 412 (411000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.013 - mae: 1.116 - mean_q: 1.359 - mean_eps: 0.630 - ale.lives: 2.058\n",
            "\n",
            "Interval 413 (412000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.014 - mae: 1.123 - mean_q: 1.369 - mean_eps: 0.629 - ale.lives: 2.430\n",
            "\n",
            "Interval 414 (413000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.014 - mae: 1.121 - mean_q: 1.368 - mean_eps: 0.628 - ale.lives: 1.475\n",
            "\n",
            "Interval 415 (414000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.015 - mae: 1.115 - mean_q: 1.362 - mean_eps: 0.627 - ale.lives: 2.119\n",
            "\n",
            "Interval 416 (415000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.017 - mae: 1.121 - mean_q: 1.368 - mean_eps: 0.626 - ale.lives: 2.471\n",
            "\n",
            "Interval 417 (416000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.015 - mae: 1.131 - mean_q: 1.380 - mean_eps: 0.625 - ale.lives: 1.784\n",
            "\n",
            "Interval 418 (417000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.014 - mae: 1.116 - mean_q: 1.359 - mean_eps: 0.624 - ale.lives: 2.813\n",
            "\n",
            "Interval 419 (418000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.000 [7.000, 23.000] - loss: 0.013 - mae: 1.125 - mean_q: 1.368 - mean_eps: 0.623 - ale.lives: 2.280\n",
            "\n",
            "Interval 420 (419000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.013 - mae: 1.114 - mean_q: 1.357 - mean_eps: 0.622 - ale.lives: 2.060\n",
            "\n",
            "Interval 421 (420000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.013 - mae: 1.141 - mean_q: 1.389 - mean_eps: 0.622 - ale.lives: 1.938\n",
            "\n",
            "Interval 422 (421000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.014 - mae: 1.170 - mean_q: 1.425 - mean_eps: 0.621 - ale.lives: 1.853\n",
            "\n",
            "Interval 423 (422000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.015 - mae: 1.160 - mean_q: 1.413 - mean_eps: 0.620 - ale.lives: 2.170\n",
            "\n",
            "Interval 424 (423000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.016 - mae: 1.162 - mean_q: 1.414 - mean_eps: 0.619 - ale.lives: 2.014\n",
            "\n",
            "Interval 425 (424000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 16.000 [9.000, 23.000] - loss: 0.015 - mae: 1.161 - mean_q: 1.414 - mean_eps: 0.618 - ale.lives: 1.731\n",
            "\n",
            "Interval 426 (425000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.020 - mae: 1.167 - mean_q: 1.421 - mean_eps: 0.617 - ale.lives: 2.416\n",
            "\n",
            "Interval 427 (426000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 6.500 [4.000, 9.000] - loss: 0.014 - mae: 1.161 - mean_q: 1.414 - mean_eps: 0.616 - ale.lives: 2.039\n",
            "\n",
            "Interval 428 (427000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.014 - mae: 1.153 - mean_q: 1.402 - mean_eps: 0.615 - ale.lives: 2.245\n",
            "\n",
            "Interval 429 (428000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 12.000 [7.000, 17.000] - loss: 0.014 - mae: 1.159 - mean_q: 1.409 - mean_eps: 0.614 - ale.lives: 2.096\n",
            "\n",
            "Interval 430 (429000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.015 - mae: 1.167 - mean_q: 1.419 - mean_eps: 0.613 - ale.lives: 2.276\n",
            "\n",
            "Interval 431 (430000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0250\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 0.014 - mae: 1.179 - mean_q: 1.438 - mean_eps: 0.613 - ale.lives: 2.364\n",
            "\n",
            "Interval 432 (431000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.015 - mae: 1.177 - mean_q: 1.435 - mean_eps: 0.612 - ale.lives: 2.092\n",
            "\n",
            "Interval 433 (432000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.016 - mae: 1.185 - mean_q: 1.446 - mean_eps: 0.611 - ale.lives: 1.926\n",
            "\n",
            "Interval 434 (433000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.000 [7.000, 19.000] - loss: 0.017 - mae: 1.180 - mean_q: 1.436 - mean_eps: 0.610 - ale.lives: 2.386\n",
            "\n",
            "Interval 435 (434000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.014 - mae: 1.161 - mean_q: 1.413 - mean_eps: 0.609 - ale.lives: 1.664\n",
            "\n",
            "Interval 436 (435000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.017 - mae: 1.189 - mean_q: 1.447 - mean_eps: 0.608 - ale.lives: 1.950\n",
            "\n",
            "Interval 437 (436000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 7.500 [3.000, 12.000] - loss: 0.015 - mae: 1.166 - mean_q: 1.417 - mean_eps: 0.607 - ale.lives: 1.918\n",
            "\n",
            "Interval 438 (437000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.015 - mae: 1.195 - mean_q: 1.454 - mean_eps: 0.606 - ale.lives: 2.183\n",
            "\n",
            "Interval 439 (438000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.015 - mae: 1.182 - mean_q: 1.439 - mean_eps: 0.605 - ale.lives: 2.107\n",
            "\n",
            "Interval 440 (439000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.015 - mae: 1.179 - mean_q: 1.435 - mean_eps: 0.604 - ale.lives: 2.203\n",
            "\n",
            "Interval 441 (440000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.019 - mae: 1.212 - mean_q: 1.476 - mean_eps: 0.604 - ale.lives: 1.873\n",
            "\n",
            "Interval 442 (441000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.016 - mae: 1.213 - mean_q: 1.481 - mean_eps: 0.603 - ale.lives: 1.895\n",
            "\n",
            "Interval 443 (442000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.015 - mae: 1.207 - mean_q: 1.468 - mean_eps: 0.602 - ale.lives: 1.937\n",
            "\n",
            "Interval 444 (443000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.500 [15.000, 16.000] - loss: 0.015 - mae: 1.221 - mean_q: 1.485 - mean_eps: 0.601 - ale.lives: 1.814\n",
            "\n",
            "Interval 445 (444000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "Interval 446 (445000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 19.000 [16.000, 22.000] - loss: 0.015 - mae: 1.217 - mean_q: 1.479 - mean_eps: 0.599 - ale.lives: 2.130\n",
            "\n",
            "Interval 447 (446000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.017 - mae: 1.225 - mean_q: 1.492 - mean_eps: 0.598 - ale.lives: 2.204\n",
            "\n",
            "Interval 448 (447000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 14.500 [12.000, 17.000] - loss: 0.015 - mae: 1.206 - mean_q: 1.468 - mean_eps: 0.597 - ale.lives: 2.290\n",
            "\n",
            "Interval 449 (448000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 6.500 [5.000, 8.000] - loss: 0.016 - mae: 1.209 - mean_q: 1.475 - mean_eps: 0.596 - ale.lives: 2.374\n",
            "\n",
            "Interval 450 (449000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.013 - mae: 1.199 - mean_q: 1.459 - mean_eps: 0.595 - ale.lives: 2.051\n",
            "\n",
            "Interval 451 (450000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.015 - mae: 1.275 - mean_q: 1.553 - mean_eps: 0.595 - ale.lives: 2.299\n",
            "\n",
            "Interval 452 (451000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.015 - mae: 1.263 - mean_q: 1.536 - mean_eps: 0.594 - ale.lives: 1.619\n",
            "\n",
            "Interval 453 (452000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.015 - mae: 1.247 - mean_q: 1.516 - mean_eps: 0.593 - ale.lives: 2.230\n",
            "\n",
            "Interval 454 (453000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.016 - mae: 1.274 - mean_q: 1.549 - mean_eps: 0.592 - ale.lives: 2.059\n",
            "\n",
            "Interval 455 (454000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.000 [8.000, 16.000] - loss: 0.018 - mae: 1.283 - mean_q: 1.560 - mean_eps: 0.591 - ale.lives: 1.783\n",
            "\n",
            "Interval 456 (455000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.015 - mae: 1.271 - mean_q: 1.545 - mean_eps: 0.590 - ale.lives: 2.428\n",
            "\n",
            "Interval 457 (456000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.014 - mae: 1.255 - mean_q: 1.527 - mean_eps: 0.589 - ale.lives: 2.195\n",
            "\n",
            "Interval 458 (457000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.022 - mae: 1.284 - mean_q: 1.563 - mean_eps: 0.588 - ale.lives: 1.982\n",
            "\n",
            "Interval 459 (458000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 12.500 [5.000, 20.000] - loss: 0.016 - mae: 1.267 - mean_q: 1.540 - mean_eps: 0.587 - ale.lives: 1.945\n",
            "\n",
            "Interval 460 (459000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.016 - mae: 1.272 - mean_q: 1.547 - mean_eps: 0.586 - ale.lives: 2.132\n",
            "\n",
            "Interval 461 (460000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.013 - mae: 1.304 - mean_q: 1.585 - mean_eps: 0.586 - ale.lives: 2.367\n",
            "\n",
            "Interval 462 (461000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.014 - mae: 1.296 - mean_q: 1.575 - mean_eps: 0.585 - ale.lives: 2.015\n",
            "\n",
            "Interval 463 (462000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.015 - mae: 1.293 - mean_q: 1.569 - mean_eps: 0.584 - ale.lives: 1.919\n",
            "\n",
            "Interval 464 (463000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.015 - mae: 1.315 - mean_q: 1.596 - mean_eps: 0.583 - ale.lives: 2.224\n",
            "\n",
            "Interval 465 (464000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.017 - mae: 1.310 - mean_q: 1.592 - mean_eps: 0.582 - ale.lives: 2.224\n",
            "\n",
            "Interval 466 (465000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.017 - mae: 1.317 - mean_q: 1.596 - mean_eps: 0.581 - ale.lives: 1.930\n",
            "\n",
            "Interval 467 (466000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.018 - mae: 1.314 - mean_q: 1.595 - mean_eps: 0.580 - ale.lives: 2.212\n",
            "\n",
            "Interval 468 (467000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.015 - mae: 1.299 - mean_q: 1.577 - mean_eps: 0.579 - ale.lives: 2.138\n",
            "\n",
            "Interval 469 (468000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.016 - mae: 1.308 - mean_q: 1.586 - mean_eps: 0.578 - ale.lives: 2.918\n",
            "\n",
            "Interval 470 (469000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.014 - mae: 1.301 - mean_q: 1.579 - mean_eps: 0.577 - ale.lives: 2.063\n",
            "\n",
            "Interval 471 (470000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.016 - mae: 1.345 - mean_q: 1.633 - mean_eps: 0.577 - ale.lives: 1.651\n",
            "\n",
            "Interval 472 (471000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.015 - mae: 1.339 - mean_q: 1.625 - mean_eps: 0.576 - ale.lives: 1.946\n",
            "\n",
            "Interval 473 (472000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 6.500 [5.000, 8.000] - loss: 0.017 - mae: 1.351 - mean_q: 1.638 - mean_eps: 0.575 - ale.lives: 2.088\n",
            "\n",
            "Interval 474 (473000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.020 - mae: 1.338 - mean_q: 1.621 - mean_eps: 0.574 - ale.lives: 1.717\n",
            "\n",
            "Interval 475 (474000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.020 - mae: 1.337 - mean_q: 1.624 - mean_eps: 0.573 - ale.lives: 2.456\n",
            "\n",
            "Interval 476 (475000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.000 [10.000, 18.000] - loss: 0.023 - mae: 1.352 - mean_q: 1.641 - mean_eps: 0.572 - ale.lives: 1.826\n",
            "\n",
            "Interval 477 (476000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.016 - mae: 1.335 - mean_q: 1.619 - mean_eps: 0.571 - ale.lives: 2.509\n",
            "\n",
            "Interval 478 (477000 steps performed)\n",
            "1000/1000 [==============================] - 25s 24ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.017 - mae: 1.349 - mean_q: 1.635 - mean_eps: 0.570 - ale.lives: 2.472\n",
            "\n",
            "Interval 479 (478000 steps performed)\n",
            "1000/1000 [==============================] - 24s 24ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.018 - mae: 1.330 - mean_q: 1.614 - mean_eps: 0.569 - ale.lives: 1.464\n",
            "\n",
            "Interval 480 (479000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.500 [10.000, 13.000] - loss: 0.022 - mae: 1.323 - mean_q: 1.606 - mean_eps: 0.568 - ale.lives: 1.641\n",
            "\n",
            "Interval 481 (480000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.016 - mae: 1.397 - mean_q: 1.696 - mean_eps: 0.568 - ale.lives: 2.367\n",
            "\n",
            "Interval 482 (481000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.017 - mae: 1.392 - mean_q: 1.687 - mean_eps: 0.567 - ale.lives: 2.045\n",
            "\n",
            "Interval 483 (482000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 16.500 [13.000, 20.000] - loss: 0.015 - mae: 1.382 - mean_q: 1.675 - mean_eps: 0.566 - ale.lives: 2.126\n",
            "\n",
            "Interval 484 (483000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.017 - mae: 1.403 - mean_q: 1.698 - mean_eps: 0.565 - ale.lives: 2.460\n",
            "\n",
            "Interval 485 (484000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.014 - mae: 1.391 - mean_q: 1.683 - mean_eps: 0.564 - ale.lives: 1.852\n",
            "\n",
            "Interval 486 (485000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.018 - mae: 1.404 - mean_q: 1.699 - mean_eps: 0.563 - ale.lives: 2.265\n",
            "\n",
            "Interval 487 (486000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.016 - mae: 1.382 - mean_q: 1.675 - mean_eps: 0.562 - ale.lives: 2.077\n",
            "\n",
            "Interval 488 (487000 steps performed)\n",
            "1000/1000 [==============================] - 25s 25ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.017 - mae: 1.389 - mean_q: 1.683 - mean_eps: 0.561 - ale.lives: 2.169\n",
            "\n",
            "Interval 489 (488000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.500 [12.000, 15.000] - loss: 0.017 - mae: 1.396 - mean_q: 1.693 - mean_eps: 0.560 - ale.lives: 2.387\n",
            "\n",
            "Interval 490 (489000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.017 - mae: 1.403 - mean_q: 1.702 - mean_eps: 0.559 - ale.lives: 2.497\n",
            "\n",
            "Interval 491 (490000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.015 - mae: 1.428 - mean_q: 1.731 - mean_eps: 0.559 - ale.lives: 2.311\n",
            "\n",
            "Interval 492 (491000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.014 - mae: 1.422 - mean_q: 1.723 - mean_eps: 0.558 - ale.lives: 2.403\n",
            "\n",
            "Interval 493 (492000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 15.500 [10.000, 21.000] - loss: 0.016 - mae: 1.431 - mean_q: 1.735 - mean_eps: 0.557 - ale.lives: 2.163\n",
            "\n",
            "Interval 494 (493000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.016 - mae: 1.427 - mean_q: 1.728 - mean_eps: 0.556 - ale.lives: 2.409\n",
            "\n",
            "Interval 495 (494000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.021 - mae: 1.406 - mean_q: 1.707 - mean_eps: 0.555 - ale.lives: 1.813\n",
            "\n",
            "Interval 496 (495000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.500 [8.000, 13.000] - loss: 0.018 - mae: 1.435 - mean_q: 1.737 - mean_eps: 0.554 - ale.lives: 2.077\n",
            "\n",
            "Interval 497 (496000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.017 - mae: 1.425 - mean_q: 1.726 - mean_eps: 0.553 - ale.lives: 1.966\n",
            "\n",
            "Interval 498 (497000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.026 - mae: 1.435 - mean_q: 1.744 - mean_eps: 0.552 - ale.lives: 2.243\n",
            "\n",
            "Interval 499 (498000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.019 - mae: 1.432 - mean_q: 1.733 - mean_eps: 0.551 - ale.lives: 2.094\n",
            "\n",
            "Interval 500 (499000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "done, took 9374.822 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "dqn.fit(\n",
        "    env,\n",
        "    callbacks=callbacks,\n",
        "    nb_steps=500000,\n",
        "    log_interval=1000,\n",
        "    visualize=False\n",
        ")\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OHYryKd1Gb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 15.000, steps: 970\n",
            "Episode 2: reward: 11.000, steps: 647\n",
            "Episode 3: reward: 18.000, steps: 1002\n",
            "Episode 4: reward: 9.000, steps: 660\n",
            "Episode 5: reward: 17.000, steps: 1269\n",
            "Episode 6: reward: 20.000, steps: 1005\n",
            "Episode 7: reward: 23.000, steps: 1107\n",
            "Episode 8: reward: 5.000, steps: 368\n",
            "Episode 9: reward: 17.000, steps: 1261\n",
            "Episode 10: reward: 27.000, steps: 1207\n"
          ]
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "results = dqn.test(env, nb_episodes=10, visualize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward: 16200\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average reward: {np.mean(results.history['episode_reward'])*1000:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
