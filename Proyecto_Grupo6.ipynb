{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: José Jesús La Casa Nieto\n",
        "*   Alumno 2:\n",
        "*   Alumno 3:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I6n7MIefJ21i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos en el directorio: \n",
            "['checkpoint', 'dqn2_SpaceInvaders-v0_log.json', 'dqn2_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn2_SpaceInvaders-v0_weights.h5f.index', 'dqn2_SpaceInvaders-v0_weights_250000.h5f.data-00000-of-00001', 'dqn2_SpaceInvaders-v0_weights_250000.h5f.index', 'dqn2_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn2_SpaceInvaders-v0_weights_500000.h5f.index', 'dqn3_SpaceInvaders-v0_log.json', 'dqn3_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn3_SpaceInvaders-v0_weights.h5f.index', 'dqn3_SpaceInvaders-v0_weights_250000.h5f.data-00000-of-00001', 'dqn3_SpaceInvaders-v0_weights_250000.h5f.index', 'dqn3_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn3_SpaceInvaders-v0_weights_500000.h5f.index', 'dqn4_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn4_SpaceInvaders-v0_weights.h5f.index', 'dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dqn_SpaceInvaders-v0_weights_250000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_250000.h5f.index', 'dqn_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_500000.h5f.index', 'Proyecto2_Grupo6.ipynb', 'Proyecto3_Grupo6.ipynb', 'Proyecto4_Grupo6.ipynb', 'Proyecto5_Grupo6.ipynb', 'Proyecto_Grupo6.ipynb', 'Proyecto_práctico.ipynb']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbVRjvHCJ8UF"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jwOE6I_KGb2a"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jGEZUcpGb2a"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de acciones disponibles: 6\n"
          ]
        }
      ],
      "source": [
        "print(\"Número de acciones disponibles: \" + str(nb_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formato de observaciones: Box(0, 255, (210, 160, 3), uint8)\n"
          ]
        }
      ],
      "source": [
        "print(\"Formato de observaciones:\", env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O4GKrfWSGb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "channels_last\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute (Permute)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 1542      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 1,816,998\n",
            "Trainable params: 1,816,998\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "print(K.image_data_format())\n",
        "\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "elif K.image_data_format() == 'channels_first':\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "else:\n",
        "    raise RuntimeError('Unknown image_dim_ordering.')\n",
        "\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "foSlxWH1Gb2b"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=100000, window_length=WINDOW_LENGTH)    # Reduzco un poco la memoria\n",
        "processor = AtariProcessor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = LinearAnnealedPolicy(\n",
        "    EpsGreedyQPolicy(),\n",
        "    attr='eps',\n",
        "    value_max=1.,\n",
        "    value_min=.1,\n",
        "    value_test=.2,\n",
        "    nb_steps=100000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "dqn = DQNAgent(\n",
        "    model=model,\n",
        "    nb_actions=nb_actions,\n",
        "    policy=policy,\n",
        "    memory=memory,\n",
        "    processor=processor,\n",
        "    nb_steps_warmup=100000,   # Aumento el número de pasos iniciales porque el entorno es complejo y así obtiene una base sólida de experiencias\n",
        "    gamma=.99,\n",
        "    target_model_update=100000,  # Mantengo las mismas actualizaciones de las redes. Valores altos suponen mayor estabilidad pero más tiempo de entrenamiento\n",
        "    train_interval=4,\n",
        "    enable_dueling_network=True,\n",
        "    dueling_type='avg'\n",
        ")\n",
        "dqn.compile(Adam(learning_rate=.1e-4), metrics=['mae'])     # Cambio en el learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pepel\\.conda\\envs\\Miar2_rl38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 6s 3ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 2.233\n",
            "\n",
            "Interval 2 (1000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - ale.lives: 2.224\n",
            "\n",
            "Interval 3 (2000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - ale.lives: 2.332\n",
            "\n",
            "Interval 4 (3000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 1.892\n",
            "\n",
            "Interval 5 (4000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - ale.lives: 1.856\n",
            "\n",
            "Interval 6 (5000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 1.544\n",
            "\n",
            "Interval 7 (6000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - ale.lives: 2.191\n",
            "\n",
            "Interval 8 (7000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 8.500 [8.000, 9.000] - ale.lives: 1.960\n",
            "\n",
            "Interval 9 (8000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 2.559\n",
            "\n",
            "Interval 10 (9000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - ale.lives: 2.199\n",
            "\n",
            "Interval 11 (10000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 1.632\n",
            "\n",
            "Interval 12 (11000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - ale.lives: 2.167\n",
            "\n",
            "Interval 13 (12000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 4.500 [2.000, 7.000] - ale.lives: 2.260\n",
            "\n",
            "Interval 14 (13000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - ale.lives: 1.930\n",
            "\n",
            "Interval 15 (14000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 8.000 [7.000, 9.000] - ale.lives: 2.299\n",
            "\n",
            "Interval 16 (15000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 6.000 [2.000, 10.000] - ale.lives: 2.263\n",
            "\n",
            "Interval 17 (16000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 11.000 [6.000, 16.000] - ale.lives: 1.733\n",
            "\n",
            "Interval 18 (17000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - ale.lives: 2.183\n",
            "\n",
            "Interval 19 (18000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - ale.lives: 2.096\n",
            "\n",
            "Interval 20 (19000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - ale.lives: 2.496\n",
            "\n",
            "Interval 21 (20000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 14.500 [7.000, 22.000] - ale.lives: 2.325\n",
            "\n",
            "Interval 22 (21000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - ale.lives: 2.144\n",
            "\n",
            "Interval 23 (22000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - ale.lives: 2.462\n",
            "\n",
            "Interval 24 (23000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - ale.lives: 1.914\n",
            "\n",
            "Interval 25 (24000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - ale.lives: 2.324\n",
            "\n",
            "Interval 26 (25000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - ale.lives: 2.383\n",
            "\n",
            "Interval 27 (26000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 2.067\n",
            "\n",
            "Interval 28 (27000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [4.000, 20.000] - ale.lives: 1.801\n",
            "\n",
            "Interval 29 (28000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - ale.lives: 2.320\n",
            "\n",
            "Interval 30 (29000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - ale.lives: 2.271\n",
            "\n",
            "Interval 31 (30000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 1.562\n",
            "\n",
            "Interval 32 (31000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 14.000 [7.000, 21.000] - ale.lives: 1.681\n",
            "\n",
            "Interval 33 (32000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0240\n",
            "Interval 34 (33000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - ale.lives: 2.075\n",
            "\n",
            "Interval 35 (34000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - ale.lives: 1.906\n",
            "\n",
            "Interval 36 (35000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.000 [7.000, 17.000] - ale.lives: 2.060\n",
            "\n",
            "Interval 37 (36000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.000 [8.000, 8.000] - ale.lives: 2.053\n",
            "\n",
            "Interval 38 (37000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.000 [10.000, 16.000] - ale.lives: 1.887\n",
            "\n",
            "Interval 39 (38000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 2.247\n",
            "\n",
            "Interval 40 (39000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0140\n",
            "3 episodes - episode_reward: 7.000 [2.000, 12.000] - ale.lives: 1.931\n",
            "\n",
            "Interval 41 (40000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - ale.lives: 2.183\n",
            "\n",
            "Interval 42 (41000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - ale.lives: 1.911\n",
            "\n",
            "Interval 43 (42000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - ale.lives: 1.905\n",
            "\n",
            "Interval 44 (43000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 18.000 [17.000, 19.000] - ale.lives: 1.936\n",
            "\n",
            "Interval 45 (44000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 1.969\n",
            "\n",
            "Interval 46 (45000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 7.500 [7.000, 8.000] - ale.lives: 2.126\n",
            "\n",
            "Interval 47 (46000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - ale.lives: 1.757\n",
            "\n",
            "Interval 48 (47000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - ale.lives: 2.089\n",
            "\n",
            "Interval 49 (48000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "3 episodes - episode_reward: 5.000 [3.000, 9.000] - ale.lives: 2.250\n",
            "\n",
            "Interval 50 (49000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - ale.lives: 2.036\n",
            "\n",
            "Interval 51 (50000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 16.500 [14.000, 19.000] - ale.lives: 1.886\n",
            "\n",
            "Interval 52 (51000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 2.265\n",
            "\n",
            "Interval 53 (52000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - ale.lives: 1.589\n",
            "\n",
            "Interval 54 (53000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - ale.lives: 1.846\n",
            "\n",
            "Interval 55 (54000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - ale.lives: 1.871\n",
            "\n",
            "Interval 56 (55000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 17.500 [12.000, 23.000] - ale.lives: 1.909\n",
            "\n",
            "Interval 57 (56000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 2.008\n",
            "\n",
            "Interval 58 (57000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 13.500 [12.000, 15.000] - ale.lives: 1.998\n",
            "\n",
            "Interval 59 (58000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 4.000 [3.000, 5.000] - ale.lives: 1.977\n",
            "\n",
            "Interval 60 (59000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0160\n",
            "3 episodes - episode_reward: 8.333 [4.000, 11.000] - ale.lives: 2.162\n",
            "\n",
            "Interval 61 (60000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - ale.lives: 2.121\n",
            "\n",
            "Interval 62 (61000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - ale.lives: 1.723\n",
            "\n",
            "Interval 63 (62000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 2.066\n",
            "\n",
            "Interval 64 (63000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 2.047\n",
            "\n",
            "Interval 65 (64000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - ale.lives: 1.789\n",
            "\n",
            "Interval 66 (65000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - ale.lives: 1.578\n",
            "\n",
            "Interval 67 (66000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.500 [12.000, 17.000] - ale.lives: 2.063\n",
            "\n",
            "Interval 68 (67000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - ale.lives: 1.721\n",
            "\n",
            "Interval 69 (68000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0250\n",
            "2 episodes - episode_reward: 16.500 [16.000, 17.000] - ale.lives: 2.181\n",
            "\n",
            "Interval 70 (69000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - ale.lives: 1.982\n",
            "\n",
            "Interval 71 (70000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - ale.lives: 1.521\n",
            "\n",
            "Interval 72 (71000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0160\n",
            "Interval 73 (72000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - ale.lives: 1.547\n",
            "\n",
            "Interval 74 (73000 steps performed)\n",
            "1000/1000 [==============================] - 2s 2ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - ale.lives: 1.837\n",
            "\n",
            "Interval 75 (74000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - ale.lives: 2.187\n",
            "\n",
            "Interval 76 (75000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - ale.lives: 2.154\n",
            "\n",
            "Interval 77 (76000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - ale.lives: 1.772\n",
            "\n",
            "Interval 78 (77000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - ale.lives: 1.866\n",
            "\n",
            "Interval 79 (78000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - ale.lives: 2.094\n",
            "\n",
            "Interval 80 (79000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.000 [11.000, 17.000] - ale.lives: 1.840\n",
            "\n",
            "Interval 81 (80000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0150\n",
            "Interval 82 (81000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - ale.lives: 1.670\n",
            "\n",
            "Interval 83 (82000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - ale.lives: 1.787\n",
            "\n",
            "Interval 84 (83000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - ale.lives: 1.807\n",
            "\n",
            "Interval 85 (84000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.500 [11.000, 18.000] - ale.lives: 2.215\n",
            "\n",
            "Interval 86 (85000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - ale.lives: 1.904\n",
            "\n",
            "Interval 87 (86000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 1.876\n",
            "\n",
            "Interval 88 (87000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 14.500 [12.000, 17.000] - ale.lives: 2.164\n",
            "\n",
            "Interval 89 (88000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - ale.lives: 1.131\n",
            "\n",
            "Interval 90 (89000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - ale.lives: 1.862\n",
            "\n",
            "Interval 91 (90000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 2.286\n",
            "\n",
            "Interval 92 (91000 steps performed)\n",
            "1000/1000 [==============================] - 3s 3ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - ale.lives: 1.882\n",
            "\n",
            "Interval 93 (92000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.000 [9.000, 19.000] - ale.lives: 2.308\n",
            "\n",
            "Interval 94 (93000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.500 [13.000, 14.000] - ale.lives: 2.032\n",
            "\n",
            "Interval 95 (94000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - ale.lives: 2.111\n",
            "\n",
            "Interval 96 (95000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - ale.lives: 2.108\n",
            "\n",
            "Interval 97 (96000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 2.043\n",
            "\n",
            "Interval 98 (97000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - ale.lives: 1.805\n",
            "\n",
            "Interval 99 (98000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 16.000 [16.000, 16.000] - ale.lives: 2.193\n",
            "\n",
            "Interval 100 (99000 steps performed)\n",
            "1000/1000 [==============================] - 4s 4ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - ale.lives: 1.899\n",
            "\n",
            "Interval 101 (100000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.500 [2.000, 17.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.184\n",
            "\n",
            "Interval 102 (101000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0060\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.573\n",
            "\n",
            "Interval 103 (102000 steps performed)\n",
            "1000/1000 [==============================] - 22s 21ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.033 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.246\n",
            "\n",
            "Interval 104 (103000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0050\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.031 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.771\n",
            "\n",
            "Interval 105 (104000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.256\n",
            "\n",
            "Interval 106 (105000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.008 - mae: 0.031 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 1.513\n",
            "\n",
            "Interval 107 (106000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.032 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.931\n",
            "\n",
            "Interval 108 (107000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 2.042\n",
            "\n",
            "Interval 109 (108000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.741\n",
            "\n",
            "Interval 110 (109000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.031 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.088\n",
            "\n",
            "Interval 111 (110000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.033 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.569\n",
            "\n",
            "Interval 112 (111000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 31.000 [31.000, 31.000] - loss: 0.008 - mae: 0.032 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.041\n",
            "\n",
            "Interval 113 (112000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.033 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.255\n",
            "\n",
            "Interval 114 (113000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0130\n",
            "Interval 115 (114000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 14.000 [6.000, 22.000] - loss: 0.008 - mae: 0.032 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.302\n",
            "\n",
            "Interval 116 (115000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.141\n",
            "\n",
            "Interval 117 (116000 steps performed)\n",
            "1000/1000 [==============================] - 22s 21ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.009 - mae: 0.034 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.540\n",
            "\n",
            "Interval 118 (117000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.033 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.155\n",
            "\n",
            "Interval 119 (118000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.010 - mae: 0.035 - mean_q: 0.045 - mean_eps: 0.100 - ale.lives: 2.079\n",
            "\n",
            "Interval 120 (119000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "Interval 121 (120000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 39.000 [39.000, 39.000] - loss: 0.009 - mae: 0.034 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 1.669\n",
            "\n",
            "Interval 122 (121000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.034 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 1.269\n",
            "\n",
            "Interval 123 (122000 steps performed)\n",
            "1000/1000 [==============================] - 20s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.633\n",
            "\n",
            "Interval 124 (123000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0150\n",
            "Interval 125 (124000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 17.500 [10.000, 25.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.566\n",
            "\n",
            "Interval 126 (125000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "Interval 127 (126000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 32.000 [32.000, 32.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.063\n",
            "\n",
            "Interval 128 (127000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.993\n",
            "\n",
            "Interval 129 (128000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 1.929\n",
            "\n",
            "Interval 130 (129000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 8.000 [4.000, 12.000] - loss: 0.009 - mae: 0.033 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 1.868\n",
            "\n",
            "Interval 131 (130000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.433\n",
            "\n",
            "Interval 132 (131000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.031 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.011\n",
            "\n",
            "Interval 133 (132000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 7.000 [5.000, 9.000] - loss: 0.009 - mae: 0.034 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.157\n",
            "\n",
            "Interval 134 (133000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.032 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.752\n",
            "\n",
            "Interval 135 (134000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "Interval 136 (135000 steps performed)\n",
            "1000/1000 [==============================] - 22s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 41.000 [41.000, 41.000] - loss: 0.008 - mae: 0.031 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.712\n",
            "\n",
            "Interval 137 (136000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 1.748\n",
            "\n",
            "Interval 138 (137000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 7.000 [5.000, 9.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.115\n",
            "\n",
            "Interval 139 (138000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 31.000 [31.000, 31.000] - loss: 0.008 - mae: 0.032 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.549\n",
            "\n",
            "Interval 140 (139000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.406\n",
            "\n",
            "Interval 141 (140000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0040\n",
            "Interval 142 (141000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.500 [8.000, 17.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 1.945\n",
            "\n",
            "Interval 143 (142000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.927\n",
            "\n",
            "Interval 144 (143000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.158\n",
            "\n",
            "Interval 145 (144000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.000 [11.000, 15.000] - loss: 0.009 - mae: 0.029 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 1.758\n",
            "\n",
            "Interval 146 (145000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.007 - mae: 0.028 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 1.575\n",
            "\n",
            "Interval 147 (146000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.197\n",
            "\n",
            "Interval 148 (147000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.324\n",
            "\n",
            "Interval 149 (148000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 36.000 [36.000, 36.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.792\n",
            "\n",
            "Interval 150 (149000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.274\n",
            "\n",
            "Interval 151 (150000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.137\n",
            "\n",
            "Interval 152 (151000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.205\n",
            "\n",
            "Interval 153 (152000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.010 - mae: 0.033 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.463\n",
            "\n",
            "Interval 154 (153000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 19.000 [14.000, 24.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.240\n",
            "\n",
            "Interval 155 (154000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.403\n",
            "\n",
            "Interval 156 (155000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.032 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.059\n",
            "\n",
            "Interval 157 (156000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.739\n",
            "\n",
            "Interval 158 (157000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0110\n",
            "3 episodes - episode_reward: 5.667 [3.000, 7.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.024\n",
            "\n",
            "Interval 159 (158000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.311\n",
            "\n",
            "Interval 160 (159000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.040\n",
            "\n",
            "Interval 161 (160000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.091\n",
            "\n",
            "Interval 162 (161000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 16.000 [12.000, 20.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.016\n",
            "\n",
            "Interval 163 (162000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 2.380\n",
            "\n",
            "Interval 164 (163000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.165\n",
            "\n",
            "Interval 165 (164000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.700\n",
            "\n",
            "Interval 166 (165000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.010 - mae: 0.031 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.178\n",
            "\n",
            "Interval 167 (166000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.812\n",
            "\n",
            "Interval 168 (167000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 10.000 [4.000, 16.000] - loss: 0.007 - mae: 0.026 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 1.928\n",
            "\n",
            "Interval 169 (168000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.299\n",
            "\n",
            "Interval 170 (169000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.687\n",
            "\n",
            "Interval 171 (170000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.029 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.240\n",
            "\n",
            "Interval 172 (171000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 2.373\n",
            "\n",
            "Interval 173 (172000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.146\n",
            "\n",
            "Interval 174 (173000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.500 [8.000, 19.000] - loss: 0.008 - mae: 0.030 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.074\n",
            "\n",
            "Interval 175 (174000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.000 [6.000, 18.000] - loss: 0.009 - mae: 0.029 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 1.974\n",
            "\n",
            "Interval 176 (175000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 2.061\n",
            "\n",
            "Interval 177 (176000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.027 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.101\n",
            "\n",
            "Interval 178 (177000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.224\n",
            "\n",
            "Interval 179 (178000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.006 - mae: 0.025 - mean_q: 0.035 - mean_eps: 0.100 - ale.lives: 1.817\n",
            "\n",
            "Interval 180 (179000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.000 [7.000, 9.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 2.125\n",
            "\n",
            "Interval 181 (180000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 4.500 [4.000, 5.000] - loss: 0.010 - mae: 0.032 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 2.120\n",
            "\n",
            "Interval 182 (181000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.331\n",
            "\n",
            "Interval 183 (182000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0290\n",
            "2 episodes - episode_reward: 15.500 [9.000, 22.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.231\n",
            "\n",
            "Interval 184 (183000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 12.000 [4.000, 20.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 2.164\n",
            "\n",
            "Interval 185 (184000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.294\n",
            "\n",
            "Interval 186 (185000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.008 - mae: 0.027 - mean_q: 0.038 - mean_eps: 0.100 - ale.lives: 2.005\n",
            "\n",
            "Interval 187 (186000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.816\n",
            "\n",
            "Interval 188 (187000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.028 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.764\n",
            "\n",
            "Interval 189 (188000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.671\n",
            "\n",
            "Interval 190 (189000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "3 episodes - episode_reward: 8.667 [5.000, 13.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 1.861\n",
            "\n",
            "Interval 191 (190000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.010 - mae: 0.032 - mean_q: 0.044 - mean_eps: 0.100 - ale.lives: 2.488\n",
            "\n",
            "Interval 192 (191000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0270\n",
            "Interval 193 (192000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 32.000 [32.000, 32.000] - loss: 0.009 - mae: 0.031 - mean_q: 0.043 - mean_eps: 0.100 - ale.lives: 1.729\n",
            "\n",
            "Interval 194 (193000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 10.000 [3.000, 17.000] - loss: 0.008 - mae: 0.027 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.334\n",
            "\n",
            "Interval 195 (194000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.030 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.748\n",
            "\n",
            "Interval 196 (195000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 20.000 [19.000, 21.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.856\n",
            "\n",
            "Interval 197 (196000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.040 - mean_eps: 0.100 - ale.lives: 1.515\n",
            "\n",
            "Interval 198 (197000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.029 - mean_q: 0.041 - mean_eps: 0.100 - ale.lives: 1.644\n",
            "\n",
            "Interval 199 (198000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.029 - mean_q: 0.042 - mean_eps: 0.100 - ale.lives: 2.149\n",
            "\n",
            "Interval 200 (199000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.008 - mae: 0.028 - mean_q: 0.039 - mean_eps: 0.100 - ale.lives: 2.610\n",
            "\n",
            "Interval 201 (200000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.078\n",
            "\n",
            "Interval 202 (201000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.011 - mae: 0.044 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 1.990\n",
            "\n",
            "Interval 203 (202000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.040 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 1.878\n",
            "\n",
            "Interval 204 (203000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 30.000 [30.000, 30.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.035\n",
            "\n",
            "Interval 205 (204000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.652\n",
            "\n",
            "Interval 206 (205000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.362\n",
            "\n",
            "Interval 207 (206000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.500 [7.000, 18.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.905\n",
            "\n",
            "Interval 208 (207000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.008 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.463\n",
            "\n",
            "Interval 209 (208000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.337\n",
            "\n",
            "Interval 210 (209000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.052\n",
            "\n",
            "Interval 211 (210000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 16.500 [16.000, 17.000] - loss: 0.007 - mae: 0.037 - mean_q: 0.053 - mean_eps: 0.100 - ale.lives: 1.883\n",
            "\n",
            "Interval 212 (211000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 1.543\n",
            "\n",
            "Interval 213 (212000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.571\n",
            "\n",
            "Interval 214 (213000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 1.844\n",
            "\n",
            "Interval 215 (214000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.010 - mae: 0.044 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.570\n",
            "\n",
            "Interval 216 (215000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.056\n",
            "\n",
            "Interval 217 (216000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 10.500 [5.000, 16.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.151\n",
            "\n",
            "Interval 218 (217000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 1.879\n",
            "\n",
            "Interval 219 (218000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.429\n",
            "\n",
            "Interval 220 (219000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.355\n",
            "\n",
            "Interval 221 (220000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.896\n",
            "\n",
            "Interval 222 (221000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 30.000 [30.000, 30.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.844\n",
            "\n",
            "Interval 223 (222000 steps performed)\n",
            "1000/1000 [==============================] - 22s 21ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.388\n",
            "\n",
            "Interval 224 (223000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.743\n",
            "\n",
            "Interval 225 (224000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.104\n",
            "\n",
            "Interval 226 (225000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.155\n",
            "\n",
            "Interval 227 (226000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.839\n",
            "\n",
            "Interval 228 (227000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 4.500 [3.000, 6.000] - loss: 0.010 - mae: 0.044 - mean_q: 0.061 - mean_eps: 0.100 - ale.lives: 2.056\n",
            "\n",
            "Interval 229 (228000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.061 - mean_eps: 0.100 - ale.lives: 2.004\n",
            "\n",
            "Interval 230 (229000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.038 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.467\n",
            "\n",
            "Interval 231 (230000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 12.000 [4.000, 20.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.138\n",
            "\n",
            "Interval 232 (231000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.034\n",
            "\n",
            "Interval 233 (232000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 20.000 [17.000, 23.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 1.875\n",
            "\n",
            "Interval 234 (233000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0290\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.038 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 1.744\n",
            "\n",
            "Interval 235 (234000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 16.000 [14.000, 18.000] - loss: 0.007 - mae: 0.038 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.057\n",
            "\n",
            "Interval 236 (235000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.747\n",
            "\n",
            "Interval 237 (236000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 30.000 [30.000, 30.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.564\n",
            "\n",
            "Interval 238 (237000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 16.000 [6.000, 26.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 1.940\n",
            "\n",
            "Interval 239 (238000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.173\n",
            "\n",
            "Interval 240 (239000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.493\n",
            "\n",
            "Interval 241 (240000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.061 - mean_eps: 0.100 - ale.lives: 1.988\n",
            "\n",
            "Interval 242 (241000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 1.424\n",
            "\n",
            "Interval 243 (242000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.927\n",
            "\n",
            "Interval 244 (243000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0250\n",
            "Interval 245 (244000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.817\n",
            "\n",
            "Interval 246 (245000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 19.000 [14.000, 24.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.415\n",
            "\n",
            "Interval 247 (246000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.247\n",
            "\n",
            "Interval 248 (247000 steps performed)\n",
            "1000/1000 [==============================] - 21s 20ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.087\n",
            "\n",
            "Interval 249 (248000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 7.500 [7.000, 8.000] - loss: 0.010 - mae: 0.044 - mean_q: 0.061 - mean_eps: 0.100 - ale.lives: 2.059\n",
            "\n",
            "Interval 250 (249000 steps performed)\n",
            "1000/1000 [==============================] - 21s 20ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.091\n",
            "\n",
            "Interval 251 (250000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.360\n",
            "\n",
            "Interval 252 (251000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 29.000 [29.000, 29.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.232\n",
            "\n",
            "Interval 253 (252000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.090\n",
            "\n",
            "Interval 254 (253000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 7.000 [3.000, 11.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.916\n",
            "\n",
            "Interval 255 (254000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.519\n",
            "\n",
            "Interval 256 (255000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.862\n",
            "\n",
            "Interval 257 (256000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.008 - mae: 0.040 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.554\n",
            "\n",
            "Interval 258 (257000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 12.500 [7.000, 18.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.062\n",
            "\n",
            "Interval 259 (258000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 7.500 [4.000, 11.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.004\n",
            "\n",
            "Interval 260 (259000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [4.000, 18.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.124\n",
            "\n",
            "Interval 261 (260000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.914\n",
            "\n",
            "Interval 262 (261000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.759\n",
            "\n",
            "Interval 263 (262000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 1.889\n",
            "\n",
            "Interval 264 (263000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.432\n",
            "\n",
            "Interval 265 (264000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 17.500 [11.000, 24.000] - loss: 0.010 - mae: 0.044 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.023\n",
            "\n",
            "Interval 266 (265000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.484\n",
            "\n",
            "Interval 267 (266000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 12.500 [12.000, 13.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.028\n",
            "\n",
            "Interval 268 (267000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.034\n",
            "\n",
            "Interval 269 (268000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.039 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.114\n",
            "\n",
            "Interval 270 (269000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.732\n",
            "\n",
            "Interval 271 (270000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.426\n",
            "\n",
            "Interval 272 (271000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.010 - mae: 0.043 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.317\n",
            "\n",
            "Interval 273 (272000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.087\n",
            "\n",
            "Interval 274 (273000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 13.000 [10.000, 16.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.906\n",
            "\n",
            "Interval 275 (274000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 11.000 [6.000, 16.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 1.961\n",
            "\n",
            "Interval 276 (275000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.023\n",
            "\n",
            "Interval 277 (276000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 1.968\n",
            "\n",
            "Interval 278 (277000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.286\n",
            "\n",
            "Interval 279 (278000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.063\n",
            "\n",
            "Interval 280 (279000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 12.000 [4.000, 20.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.105\n",
            "\n",
            "Interval 281 (280000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0270\n",
            "2 episodes - episode_reward: 15.000 [13.000, 17.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 1.946\n",
            "\n",
            "Interval 282 (281000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.537\n",
            "\n",
            "Interval 283 (282000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.128\n",
            "\n",
            "Interval 284 (283000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.161\n",
            "\n",
            "Interval 285 (284000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.055 - mean_eps: 0.100 - ale.lives: 2.137\n",
            "\n",
            "Interval 286 (285000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 1.639\n",
            "\n",
            "Interval 287 (286000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 2.000 [2.000, 2.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.681\n",
            "\n",
            "Interval 288 (287000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.349\n",
            "\n",
            "Interval 289 (288000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.000 [8.000, 22.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.194\n",
            "\n",
            "Interval 290 (289000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.078\n",
            "\n",
            "Interval 291 (290000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 8.000 [3.000, 13.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.058 - mean_eps: 0.100 - ale.lives: 2.001\n",
            "\n",
            "Interval 292 (291000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.040 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.092\n",
            "\n",
            "Interval 293 (292000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.039 - mean_q: 0.056 - mean_eps: 0.100 - ale.lives: 2.322\n",
            "\n",
            "Interval 294 (293000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 11.500 [3.000, 20.000] - loss: 0.010 - mae: 0.042 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 2.110\n",
            "\n",
            "Interval 295 (294000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.011 - mae: 0.045 - mean_q: 0.062 - mean_eps: 0.100 - ale.lives: 2.204\n",
            "\n",
            "Interval 296 (295000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 2.216\n",
            "\n",
            "Interval 297 (296000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.041 - mean_q: 0.057 - mean_eps: 0.100 - ale.lives: 2.336\n",
            "\n",
            "Interval 298 (297000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.010 - mae: 0.044 - mean_q: 0.062 - mean_eps: 0.100 - ale.lives: 2.178\n",
            "\n",
            "Interval 299 (298000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.060 - mean_eps: 0.100 - ale.lives: 1.953\n",
            "\n",
            "Interval 300 (299000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.042 - mean_q: 0.059 - mean_eps: 0.100 - ale.lives: 1.999\n",
            "\n",
            "Interval 301 (300000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.010 - mae: 0.062 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.549\n",
            "\n",
            "Interval 302 (301000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.010 - mae: 0.064 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.858\n",
            "\n",
            "Interval 303 (302000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.009 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.957\n",
            "\n",
            "Interval 304 (303000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0170\n",
            "3 episodes - episode_reward: 8.000 [6.000, 11.000] - loss: 0.010 - mae: 0.063 - mean_q: 0.087 - mean_eps: 0.100 - ale.lives: 2.074\n",
            "\n",
            "Interval 305 (304000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "Interval 306 (305000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 15.000 [5.000, 25.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.145\n",
            "\n",
            "Interval 307 (306000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.564\n",
            "\n",
            "Interval 308 (307000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.059 - mean_q: 0.082 - mean_eps: 0.100 - ale.lives: 1.829\n",
            "\n",
            "Interval 309 (308000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.253\n",
            "\n",
            "Interval 310 (309000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.007\n",
            "\n",
            "Interval 311 (310000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "Interval 312 (311000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 25.500 [21.000, 30.000] - loss: 0.009 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.331\n",
            "\n",
            "Interval 313 (312000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.242\n",
            "\n",
            "Interval 314 (313000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.854\n",
            "\n",
            "Interval 315 (314000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.010 - mae: 0.062 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.137\n",
            "\n",
            "Interval 316 (315000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.449\n",
            "\n",
            "Interval 317 (316000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.089\n",
            "\n",
            "Interval 318 (317000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.020\n",
            "\n",
            "Interval 319 (318000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 12.500 [6.000, 19.000] - loss: 0.008 - mae: 0.059 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.777\n",
            "\n",
            "Interval 320 (319000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 19.500 [16.000, 23.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.953\n",
            "\n",
            "Interval 321 (320000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.954\n",
            "\n",
            "Interval 322 (321000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 12.000 [11.000, 13.000] - loss: 0.008 - mae: 0.059 - mean_q: 0.082 - mean_eps: 0.100 - ale.lives: 1.821\n",
            "\n",
            "Interval 323 (322000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.295\n",
            "\n",
            "Interval 324 (323000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 13.500 [7.000, 20.000] - loss: 0.009 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.941\n",
            "\n",
            "Interval 325 (324000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [9.000, 15.000] - loss: 0.010 - mae: 0.062 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.389\n",
            "\n",
            "Interval 326 (325000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0250\n",
            "Interval 327 (326000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 19.000 [11.000, 27.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.699\n",
            "\n",
            "Interval 328 (327000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.010 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.998\n",
            "\n",
            "Interval 329 (328000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.010 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.679\n",
            "\n",
            "Interval 330 (329000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.070\n",
            "\n",
            "Interval 331 (330000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.000 [7.000, 15.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.682\n",
            "\n",
            "Interval 332 (331000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "Interval 333 (332000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.000 [2.000, 20.000] - loss: 0.007 - mae: 0.057 - mean_q: 0.081 - mean_eps: 0.100 - ale.lives: 2.200\n",
            "\n",
            "Interval 334 (333000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.010 - mae: 0.063 - mean_q: 0.087 - mean_eps: 0.100 - ale.lives: 2.237\n",
            "\n",
            "Interval 335 (334000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.999\n",
            "\n",
            "Interval 336 (335000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.010 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.971\n",
            "\n",
            "Interval 337 (336000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 14.000 [11.000, 17.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.066\n",
            "\n",
            "Interval 338 (337000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "Interval 339 (338000 steps performed)\n",
            "1000/1000 [==============================] - 20s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 13.000 [5.000, 21.000] - loss: 0.009 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.916\n",
            "\n",
            "Interval 340 (339000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 5.500 [2.000, 9.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.875\n",
            "\n",
            "Interval 341 (340000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.010 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.142\n",
            "\n",
            "Interval 342 (341000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0260\n",
            "2 episodes - episode_reward: 11.000 [6.000, 16.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.483\n",
            "\n",
            "Interval 343 (342000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.263\n",
            "\n",
            "Interval 344 (343000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.934\n",
            "\n",
            "Interval 345 (344000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.593\n",
            "\n",
            "Interval 346 (345000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.010 - mae: 0.065 - mean_q: 0.088 - mean_eps: 0.100 - ale.lives: 1.691\n",
            "\n",
            "Interval 347 (346000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.787\n",
            "\n",
            "Interval 348 (347000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - loss: 0.009 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.156\n",
            "\n",
            "Interval 349 (348000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.863\n",
            "\n",
            "Interval 350 (349000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.455\n",
            "\n",
            "Interval 351 (350000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 18.000 [12.000, 24.000] - loss: 0.008 - mae: 0.061 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.146\n",
            "\n",
            "Interval 352 (351000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.121\n",
            "\n",
            "Interval 353 (352000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.579\n",
            "\n",
            "Interval 354 (353000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.373\n",
            "\n",
            "Interval 355 (354000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 13.500 [11.000, 16.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.797\n",
            "\n",
            "Interval 356 (355000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.888\n",
            "\n",
            "Interval 357 (356000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.893\n",
            "\n",
            "Interval 358 (357000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 15.000 [7.000, 23.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.824\n",
            "\n",
            "Interval 359 (358000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.059 - mean_q: 0.082 - mean_eps: 0.100 - ale.lives: 2.495\n",
            "\n",
            "Interval 360 (359000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0290\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.010 - mae: 0.064 - mean_q: 0.088 - mean_eps: 0.100 - ale.lives: 2.214\n",
            "\n",
            "Interval 361 (360000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "Interval 362 (361000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 35.000 [35.000, 35.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.196\n",
            "\n",
            "Interval 363 (362000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 14.000 [10.000, 18.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.100\n",
            "\n",
            "Interval 364 (363000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.059 - mean_q: 0.082 - mean_eps: 0.100 - ale.lives: 1.939\n",
            "\n",
            "Interval 365 (364000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "Interval 366 (365000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.500 [12.000, 17.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.413\n",
            "\n",
            "Interval 367 (366000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.638\n",
            "\n",
            "Interval 368 (367000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.853\n",
            "\n",
            "Interval 369 (368000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.010\n",
            "\n",
            "Interval 370 (369000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.471\n",
            "\n",
            "Interval 371 (370000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.944\n",
            "\n",
            "Interval 372 (371000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.461\n",
            "\n",
            "Interval 373 (372000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0270\n",
            "2 episodes - episode_reward: 17.500 [16.000, 19.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.087 - mean_eps: 0.100 - ale.lives: 2.210\n",
            "\n",
            "Interval 374 (373000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.110\n",
            "\n",
            "Interval 375 (374000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.008 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.932\n",
            "\n",
            "Interval 376 (375000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.500 [7.000, 12.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.708\n",
            "\n",
            "Interval 377 (376000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.011 - mae: 0.067 - mean_q: 0.090 - mean_eps: 0.100 - ale.lives: 2.440\n",
            "\n",
            "Interval 378 (377000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.369\n",
            "\n",
            "Interval 379 (378000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.010 - mae: 0.065 - mean_q: 0.088 - mean_eps: 0.100 - ale.lives: 1.905\n",
            "\n",
            "Interval 380 (379000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.923\n",
            "\n",
            "Interval 381 (380000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.058 - mean_q: 0.082 - mean_eps: 0.100 - ale.lives: 2.157\n",
            "\n",
            "Interval 382 (381000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 13.500 [6.000, 21.000] - loss: 0.011 - mae: 0.066 - mean_q: 0.090 - mean_eps: 0.100 - ale.lives: 2.177\n",
            "\n",
            "Interval 383 (382000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0190\n",
            "Interval 384 (383000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 14.500 [5.000, 24.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.007\n",
            "\n",
            "Interval 385 (384000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.572\n",
            "\n",
            "Interval 386 (385000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.663\n",
            "\n",
            "Interval 387 (386000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.157\n",
            "\n",
            "Interval 388 (387000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 15.500 [10.000, 21.000] - loss: 0.009 - mae: 0.061 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.725\n",
            "\n",
            "Interval 389 (388000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "Interval 390 (389000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.008 - mae: 0.061 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 1.798\n",
            "\n",
            "Interval 391 (390000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 19.000 [18.000, 20.000] - loss: 0.009 - mae: 0.062 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 2.050\n",
            "\n",
            "Interval 392 (391000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.084 - mean_eps: 0.100 - ale.lives: 2.387\n",
            "\n",
            "Interval 393 (392000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 10.500 [4.000, 17.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 1.931\n",
            "\n",
            "Interval 394 (393000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 6.000 [5.000, 7.000] - loss: 0.010 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.760\n",
            "\n",
            "Interval 395 (394000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.100 - ale.lives: 2.323\n",
            "\n",
            "Interval 396 (395000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.059 - mean_q: 0.081 - mean_eps: 0.100 - ale.lives: 1.826\n",
            "\n",
            "Interval 397 (396000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.063 - mean_q: 0.085 - mean_eps: 0.100 - ale.lives: 1.593\n",
            "\n",
            "Interval 398 (397000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.007 - mae: 0.058 - mean_q: 0.081 - mean_eps: 0.100 - ale.lives: 1.907\n",
            "\n",
            "Interval 399 (398000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.010 - mae: 0.064 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 2.066\n",
            "\n",
            "Interval 400 (399000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.100 - ale.lives: 1.780\n",
            "\n",
            "Interval 401 (400000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 10.500 [6.000, 15.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.112 - mean_eps: 0.100 - ale.lives: 2.179\n",
            "\n",
            "Interval 402 (401000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.932\n",
            "\n",
            "Interval 403 (402000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.008 - mae: 0.082 - mean_q: 0.111 - mean_eps: 0.100 - ale.lives: 1.807\n",
            "\n",
            "Interval 404 (403000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.111 - mean_eps: 0.100 - ale.lives: 2.203\n",
            "\n",
            "Interval 405 (404000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.010 - mae: 0.083 - mean_q: 0.112 - mean_eps: 0.100 - ale.lives: 1.848\n",
            "\n",
            "Interval 406 (405000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.010 - mae: 0.084 - mean_q: 0.114 - mean_eps: 0.100 - ale.lives: 2.525\n",
            "\n",
            "Interval 407 (406000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.111 - mean_eps: 0.100 - ale.lives: 1.975\n",
            "\n",
            "Interval 408 (407000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.026\n",
            "\n",
            "Interval 409 (408000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.083 - mean_q: 0.113 - mean_eps: 0.100 - ale.lives: 2.025\n",
            "\n",
            "Interval 410 (409000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.000 [8.000, 10.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.326\n",
            "\n",
            "Interval 411 (410000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 4.000 [3.000, 5.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.060\n",
            "\n",
            "Interval 412 (411000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 1.795\n",
            "\n",
            "Interval 413 (412000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 16.500 [16.000, 17.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.034\n",
            "\n",
            "Interval 414 (413000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0080\n",
            "2 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.053\n",
            "\n",
            "Interval 415 (414000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "Interval 416 (415000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 33.000 [33.000, 33.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.744\n",
            "\n",
            "Interval 417 (416000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.279\n",
            "\n",
            "Interval 418 (417000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.007 - mae: 0.077 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 1.332\n",
            "\n",
            "Interval 419 (418000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.112 - mean_eps: 0.100 - ale.lives: 1.707\n",
            "\n",
            "Interval 420 (419000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 5.500 [5.000, 6.000] - loss: 0.009 - mae: 0.080 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.155\n",
            "\n",
            "Interval 421 (420000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 1.796\n",
            "\n",
            "Interval 422 (421000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [3.000, 15.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.165\n",
            "\n",
            "Interval 423 (422000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.980\n",
            "\n",
            "Interval 424 (423000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.500 [7.000, 10.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.078\n",
            "\n",
            "Interval 425 (424000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [6.000, 18.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.157\n",
            "\n",
            "Interval 426 (425000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.010\n",
            "\n",
            "Interval 427 (426000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.537\n",
            "\n",
            "Interval 428 (427000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 13.500 [4.000, 23.000] - loss: 0.008 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.236\n",
            "\n",
            "Interval 429 (428000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.100\n",
            "\n",
            "Interval 430 (429000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.041\n",
            "\n",
            "Interval 431 (430000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.543\n",
            "\n",
            "Interval 432 (431000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.077 - mean_q: 0.106 - mean_eps: 0.100 - ale.lives: 2.136\n",
            "\n",
            "Interval 433 (432000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.007 - mae: 0.077 - mean_q: 0.106 - mean_eps: 0.100 - ale.lives: 1.998\n",
            "\n",
            "Interval 434 (433000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.383\n",
            "\n",
            "Interval 435 (434000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.078 - mean_q: 0.106 - mean_eps: 0.100 - ale.lives: 2.710\n",
            "\n",
            "Interval 436 (435000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 0.009 - mae: 0.083 - mean_q: 0.113 - mean_eps: 0.100 - ale.lives: 2.313\n",
            "\n",
            "Interval 437 (436000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.707\n",
            "\n",
            "Interval 438 (437000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 14.000 [9.000, 19.000] - loss: 0.010 - mae: 0.084 - mean_q: 0.114 - mean_eps: 0.100 - ale.lives: 1.663\n",
            "\n",
            "Interval 439 (438000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.315\n",
            "\n",
            "Interval 440 (439000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.029\n",
            "\n",
            "Interval 441 (440000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 13.000 [6.000, 20.000] - loss: 0.008 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 1.974\n",
            "\n",
            "Interval 442 (441000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.171\n",
            "\n",
            "Interval 443 (442000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.500 [4.000, 15.000] - loss: 0.009 - mae: 0.083 - mean_q: 0.112 - mean_eps: 0.100 - ale.lives: 2.007\n",
            "\n",
            "Interval 444 (443000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.771\n",
            "\n",
            "Interval 445 (444000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.083 - mean_q: 0.113 - mean_eps: 0.100 - ale.lives: 2.021\n",
            "\n",
            "Interval 446 (445000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.846\n",
            "\n",
            "Interval 447 (446000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.500 [3.000, 16.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.014\n",
            "\n",
            "Interval 448 (447000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.435\n",
            "\n",
            "Interval 449 (448000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.007 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.081\n",
            "\n",
            "Interval 450 (449000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.401\n",
            "\n",
            "Interval 451 (450000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "Interval 452 (451000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.000 [4.000, 22.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.491\n",
            "\n",
            "Interval 453 (452000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.000 [11.000, 13.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.991\n",
            "\n",
            "Interval 454 (453000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.984\n",
            "\n",
            "Interval 455 (454000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 12.500 [9.000, 16.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 1.982\n",
            "\n",
            "Interval 456 (455000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.450\n",
            "\n",
            "Interval 457 (456000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 1.484\n",
            "\n",
            "Interval 458 (457000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.500 [8.000, 11.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.163\n",
            "\n",
            "Interval 459 (458000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.077\n",
            "\n",
            "Interval 460 (459000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 11.000 [7.000, 15.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 1.963\n",
            "\n",
            "Interval 461 (460000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.077 - mean_q: 0.105 - mean_eps: 0.100 - ale.lives: 2.158\n",
            "\n",
            "Interval 462 (461000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.143\n",
            "\n",
            "Interval 463 (462000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.000 [13.000, 15.000] - loss: 0.010 - mae: 0.085 - mean_q: 0.113 - mean_eps: 0.100 - ale.lives: 1.711\n",
            "\n",
            "Interval 464 (463000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.090\n",
            "\n",
            "Interval 465 (464000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 13.000 [8.000, 18.000] - loss: 0.007 - mae: 0.077 - mean_q: 0.105 - mean_eps: 0.100 - ale.lives: 1.985\n",
            "\n",
            "Interval 466 (465000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 0.078 - mean_q: 0.106 - mean_eps: 0.100 - ale.lives: 2.257\n",
            "\n",
            "Interval 467 (466000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.007 - mae: 0.077 - mean_q: 0.105 - mean_eps: 0.100 - ale.lives: 1.870\n",
            "\n",
            "Interval 468 (467000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 1.987\n",
            "\n",
            "Interval 469 (468000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.080 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.110\n",
            "\n",
            "Interval 470 (469000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.083 - mean_q: 0.112 - mean_eps: 0.100 - ale.lives: 1.985\n",
            "\n",
            "Interval 471 (470000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 13.000 [6.000, 20.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.645\n",
            "\n",
            "Interval 472 (471000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.157\n",
            "\n",
            "Interval 473 (472000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 1.867\n",
            "\n",
            "Interval 474 (473000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 2.109\n",
            "\n",
            "Interval 475 (474000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 1.684\n",
            "\n",
            "Interval 476 (475000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.289\n",
            "\n",
            "Interval 477 (476000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.236\n",
            "\n",
            "Interval 478 (477000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 13.000 [3.000, 23.000] - loss: 0.009 - mae: 0.082 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.012\n",
            "\n",
            "Interval 479 (478000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 4.500 [4.000, 5.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.099\n",
            "\n",
            "Interval 480 (479000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.000 [4.000, 16.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.210\n",
            "\n",
            "Interval 481 (480000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.761\n",
            "\n",
            "Interval 482 (481000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.010 - mae: 0.084 - mean_q: 0.110 - mean_eps: 0.100 - ale.lives: 2.438\n",
            "\n",
            "Interval 483 (482000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "Interval 484 (483000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0340\n",
            "1 episodes - episode_reward: 32.000 [32.000, 32.000] - loss: 0.008 - mae: 0.082 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.883\n",
            "\n",
            "Interval 485 (484000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 18.500 [3.000, 34.000] - loss: 0.009 - mae: 0.081 - mean_q: 0.109 - mean_eps: 0.100 - ale.lives: 1.894\n",
            "\n",
            "Interval 486 (485000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.500 [11.000, 18.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 1.650\n",
            "\n",
            "Interval 487 (486000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.261\n",
            "\n",
            "Interval 488 (487000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.007 - mae: 0.077 - mean_q: 0.104 - mean_eps: 0.100 - ale.lives: 2.516\n",
            "\n",
            "Interval 489 (488000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.106 - mean_eps: 0.100 - ale.lives: 1.575\n",
            "\n",
            "Interval 490 (489000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.000 [10.000, 18.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.397\n",
            "\n",
            "Interval 491 (490000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "Interval 492 (491000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.079 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.119\n",
            "\n",
            "Interval 493 (492000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.009 - mae: 0.080 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 1.806\n",
            "\n",
            "Interval 494 (493000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.078 - mean_q: 0.105 - mean_eps: 0.100 - ale.lives: 1.991\n",
            "\n",
            "Interval 495 (494000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.007 - mae: 0.079 - mean_q: 0.105 - mean_eps: 0.100 - ale.lives: 2.359\n",
            "\n",
            "Interval 496 (495000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 14.000 [5.000, 23.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 2.189\n",
            "\n",
            "Interval 497 (496000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.078 - mean_q: 0.104 - mean_eps: 0.100 - ale.lives: 2.449\n",
            "\n",
            "Interval 498 (497000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.081 - mean_q: 0.108 - mean_eps: 0.100 - ale.lives: 1.851\n",
            "\n",
            "Interval 499 (498000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 2.378\n",
            "\n",
            "Interval 500 (499000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.080 - mean_q: 0.107 - mean_eps: 0.100 - ale.lives: 1.706\n",
            "\n",
            "Interval 501 (500000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.104 - mean_q: 0.138 - mean_eps: 0.100 - ale.lives: 1.823\n",
            "\n",
            "Interval 502 (501000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 15.000 [7.000, 23.000] - loss: 0.008 - mae: 0.104 - mean_q: 0.139 - mean_eps: 0.100 - ale.lives: 2.182\n",
            "\n",
            "Interval 503 (502000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.107 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 2.041\n",
            "\n",
            "Interval 504 (503000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "Interval 505 (504000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 32.000 [32.000, 32.000] - loss: 0.009 - mae: 0.105 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 1.898\n",
            "\n",
            "Interval 506 (505000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 13.500 [6.000, 21.000] - loss: 0.007 - mae: 0.103 - mean_q: 0.137 - mean_eps: 0.100 - ale.lives: 1.982\n",
            "\n",
            "Interval 507 (506000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 10.000 [9.000, 11.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 1.876\n",
            "\n",
            "Interval 508 (507000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.202\n",
            "\n",
            "Interval 509 (508000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 2.087\n",
            "\n",
            "Interval 510 (509000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.199\n",
            "\n",
            "Interval 511 (510000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 35.000 [35.000, 35.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 1.736\n",
            "\n",
            "Interval 512 (511000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "Interval 513 (512000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 15.500 [10.000, 21.000] - loss: 0.009 - mae: 0.106 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 2.165\n",
            "\n",
            "Interval 514 (513000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0310\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.007 - mae: 0.102 - mean_q: 0.138 - mean_eps: 0.100 - ale.lives: 2.253\n",
            "\n",
            "Interval 515 (514000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.500 [12.000, 19.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.069\n",
            "\n",
            "Interval 516 (515000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.009 - mae: 0.107 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 2.004\n",
            "\n",
            "Interval 517 (516000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.492\n",
            "\n",
            "Interval 518 (517000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.382\n",
            "\n",
            "Interval 519 (518000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 1.921\n",
            "\n",
            "Interval 520 (519000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 10.500 [10.000, 11.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.911\n",
            "\n",
            "Interval 521 (520000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.959\n",
            "\n",
            "Interval 522 (521000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.010 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.054\n",
            "\n",
            "Interval 523 (522000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 2.398\n",
            "\n",
            "Interval 524 (523000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 29.000 [29.000, 29.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.168\n",
            "\n",
            "Interval 525 (524000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 1.617\n",
            "\n",
            "Interval 526 (525000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 5.500 [5.000, 6.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.147\n",
            "\n",
            "Interval 527 (526000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 5.500 [4.000, 7.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.201\n",
            "\n",
            "Interval 528 (527000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.288\n",
            "\n",
            "Interval 529 (528000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 1.622\n",
            "\n",
            "Interval 530 (529000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.130\n",
            "\n",
            "Interval 531 (530000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.500 [10.000, 21.000] - loss: 0.010 - mae: 0.111 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.368\n",
            "\n",
            "Interval 532 (531000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.170\n",
            "\n",
            "Interval 533 (532000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.015\n",
            "\n",
            "Interval 534 (533000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.000 [9.000, 17.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.243\n",
            "\n",
            "Interval 535 (534000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.007 - mae: 0.104 - mean_q: 0.138 - mean_eps: 0.100 - ale.lives: 2.201\n",
            "\n",
            "Interval 536 (535000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.500 [5.000, 12.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 2.118\n",
            "\n",
            "Interval 537 (536000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 1.931\n",
            "\n",
            "Interval 538 (537000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "Interval 539 (538000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.646\n",
            "\n",
            "Interval 540 (539000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.000 [6.000, 18.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.097\n",
            "\n",
            "Interval 541 (540000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 6.500 [4.000, 9.000] - loss: 0.009 - mae: 0.106 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.104\n",
            "\n",
            "Interval 542 (541000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "Interval 543 (542000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 31.000 [31.000, 31.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.085\n",
            "\n",
            "Interval 544 (543000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 1.883\n",
            "\n",
            "Interval 545 (544000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.781\n",
            "\n",
            "Interval 546 (545000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.010 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.982\n",
            "\n",
            "Interval 547 (546000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.007 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.068\n",
            "\n",
            "Interval 548 (547000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.479\n",
            "\n",
            "Interval 549 (548000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 1.820\n",
            "\n",
            "Interval 550 (549000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 17.500 [17.000, 18.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.083\n",
            "\n",
            "Interval 551 (550000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.211\n",
            "\n",
            "Interval 552 (551000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.330\n",
            "\n",
            "Interval 553 (552000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0090\n",
            "2 episodes - episode_reward: 8.500 [2.000, 15.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 2.190\n",
            "\n",
            "Interval 554 (553000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "Interval 555 (554000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 2.269\n",
            "\n",
            "Interval 556 (555000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.362\n",
            "\n",
            "Interval 557 (556000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.022\n",
            "\n",
            "Interval 558 (557000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.855\n",
            "\n",
            "Interval 559 (558000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.107 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 2.220\n",
            "\n",
            "Interval 560 (559000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 17.000 [16.000, 18.000] - loss: 0.010 - mae: 0.111 - mean_q: 0.147 - mean_eps: 0.100 - ale.lives: 1.894\n",
            "\n",
            "Interval 561 (560000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 2.193\n",
            "\n",
            "Interval 562 (561000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.042\n",
            "\n",
            "Interval 563 (562000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 14.500 [11.000, 18.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 1.652\n",
            "\n",
            "Interval 564 (563000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.007 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 2.718\n",
            "\n",
            "Interval 565 (564000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0100\n",
            "Interval 566 (565000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 15.000 [10.000, 20.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 2.236\n",
            "\n",
            "Interval 567 (566000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.111 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.531\n",
            "\n",
            "Interval 568 (567000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 1.782\n",
            "\n",
            "Interval 569 (568000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.614\n",
            "\n",
            "Interval 570 (569000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "3 episodes - episode_reward: 6.667 [3.000, 13.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.261\n",
            "\n",
            "Interval 571 (570000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 2.357\n",
            "\n",
            "Interval 572 (571000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 1.898\n",
            "\n",
            "Interval 573 (572000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 1.755\n",
            "\n",
            "Interval 574 (573000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.794\n",
            "\n",
            "Interval 575 (574000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 1.639\n",
            "\n",
            "Interval 576 (575000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.010 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.836\n",
            "\n",
            "Interval 577 (576000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 14.000 [13.000, 15.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.148 - mean_eps: 0.100 - ale.lives: 1.715\n",
            "\n",
            "Interval 578 (577000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 7.000 [5.000, 9.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.977\n",
            "\n",
            "Interval 579 (578000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 2.330\n",
            "\n",
            "Interval 580 (579000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 1.867\n",
            "\n",
            "Interval 581 (580000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.734\n",
            "\n",
            "Interval 582 (581000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.069\n",
            "\n",
            "Interval 583 (582000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 1.418\n",
            "\n",
            "Interval 584 (583000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.756\n",
            "\n",
            "Interval 585 (584000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.008 - mae: 0.105 - mean_q: 0.141 - mean_eps: 0.100 - ale.lives: 1.921\n",
            "\n",
            "Interval 586 (585000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.009 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.798\n",
            "\n",
            "Interval 587 (586000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 11.500 [6.000, 17.000] - loss: 0.009 - mae: 0.108 - mean_q: 0.145 - mean_eps: 0.100 - ale.lives: 2.071\n",
            "\n",
            "Interval 588 (587000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.107 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 1.835\n",
            "\n",
            "Interval 589 (588000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 15.000 [11.000, 19.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.620\n",
            "\n",
            "Interval 590 (589000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.110 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.786\n",
            "\n",
            "Interval 591 (590000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 1.968\n",
            "\n",
            "Interval 592 (591000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 4.000 [3.000, 5.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 2.199\n",
            "\n",
            "Interval 593 (592000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.008 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 1.940\n",
            "\n",
            "Interval 594 (593000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.008 - mae: 0.106 - mean_q: 0.140 - mean_eps: 0.100 - ale.lives: 1.681\n",
            "\n",
            "Interval 595 (594000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 2.072\n",
            "\n",
            "Interval 596 (595000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 1.953\n",
            "\n",
            "Interval 597 (596000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.108 - mean_q: 0.143 - mean_eps: 0.100 - ale.lives: 1.654\n",
            "\n",
            "Interval 598 (597000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.111 - mean_q: 0.146 - mean_eps: 0.100 - ale.lives: 1.958\n",
            "\n",
            "Interval 599 (598000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.009 - mae: 0.109 - mean_q: 0.144 - mean_eps: 0.100 - ale.lives: 1.813\n",
            "\n",
            "Interval 600 (599000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.107 - mean_q: 0.142 - mean_eps: 0.100 - ale.lives: 1.448\n",
            "\n",
            "Interval 601 (600000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.593\n",
            "\n",
            "Interval 602 (601000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "Interval 603 (602000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 2.166\n",
            "\n",
            "Interval 604 (603000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.131 - mean_q: 0.175 - mean_eps: 0.100 - ale.lives: 1.926\n",
            "\n",
            "Interval 605 (604000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.132 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 2.085\n",
            "\n",
            "Interval 606 (605000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "3 episodes - episode_reward: 11.667 [4.000, 24.000] - loss: 0.009 - mae: 0.131 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 2.263\n",
            "\n",
            "Interval 607 (606000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.131 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 2.105\n",
            "\n",
            "Interval 608 (607000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.610\n",
            "\n",
            "Interval 609 (608000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 8.500 [7.000, 10.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 2.078\n",
            "\n",
            "Interval 610 (609000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.500 [8.000, 11.000] - loss: 0.009 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.988\n",
            "\n",
            "Interval 611 (610000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.955\n",
            "\n",
            "Interval 612 (611000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.132 - mean_q: 0.176 - mean_eps: 0.100 - ale.lives: 2.103\n",
            "\n",
            "Interval 613 (612000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 14.000 [10.000, 18.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.990\n",
            "\n",
            "Interval 614 (613000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.125\n",
            "\n",
            "Interval 615 (614000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.009 - mae: 0.131 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 2.161\n",
            "\n",
            "Interval 616 (615000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.165\n",
            "\n",
            "Interval 617 (616000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.133 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 1.974\n",
            "\n",
            "Interval 618 (617000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 1.839\n",
            "\n",
            "Interval 619 (618000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 8.500 [7.000, 10.000] - loss: 0.009 - mae: 0.133 - mean_q: 0.175 - mean_eps: 0.100 - ale.lives: 1.791\n",
            "\n",
            "Interval 620 (619000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 2.197\n",
            "\n",
            "Interval 621 (620000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.812\n",
            "\n",
            "Interval 622 (621000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.009 - mae: 0.131 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 2.183\n",
            "\n",
            "Interval 623 (622000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.009\n",
            "\n",
            "Interval 624 (623000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 33.000 [33.000, 33.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.902\n",
            "\n",
            "Interval 625 (624000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.017\n",
            "\n",
            "Interval 626 (625000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 9.500 [4.000, 15.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 2.159\n",
            "\n",
            "Interval 627 (626000 steps performed)\n",
            "1000/1000 [==============================] - 16s 16ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.844\n",
            "\n",
            "Interval 628 (627000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.132 - mean_q: 0.175 - mean_eps: 0.100 - ale.lives: 1.842\n",
            "\n",
            "Interval 629 (628000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "Interval 630 (629000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 12.000 [8.000, 16.000] - loss: 0.009 - mae: 0.133 - mean_q: 0.176 - mean_eps: 0.100 - ale.lives: 2.143\n",
            "\n",
            "Interval 631 (630000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.007 - mae: 0.127 - mean_q: 0.169 - mean_eps: 0.100 - ale.lives: 1.919\n",
            "\n",
            "Interval 632 (631000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 11.000 [8.000, 14.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.193\n",
            "\n",
            "Interval 633 (632000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.913\n",
            "\n",
            "Interval 634 (633000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 7.000 [6.000, 8.000] - loss: 0.009 - mae: 0.132 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 2.063\n",
            "\n",
            "Interval 635 (634000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.144\n",
            "\n",
            "Interval 636 (635000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.007 - mae: 0.130 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.028\n",
            "\n",
            "Interval 637 (636000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.199\n",
            "\n",
            "Interval 638 (637000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 15.000 [6.000, 24.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 1.951\n",
            "\n",
            "Interval 639 (638000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.133 - mean_q: 0.175 - mean_eps: 0.100 - ale.lives: 2.079\n",
            "\n",
            "Interval 640 (639000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.669\n",
            "\n",
            "Interval 641 (640000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.712\n",
            "\n",
            "Interval 642 (641000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 1.505\n",
            "\n",
            "Interval 643 (642000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0250\n",
            "Interval 644 (643000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 31.000 [31.000, 31.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 2.111\n",
            "\n",
            "Interval 645 (644000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.413\n",
            "\n",
            "Interval 646 (645000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.010 - mae: 0.134 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 2.167\n",
            "\n",
            "Interval 647 (646000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.797\n",
            "\n",
            "Interval 648 (647000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 7.000 [5.000, 9.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.068\n",
            "\n",
            "Interval 649 (648000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.009 - mae: 0.131 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 1.861\n",
            "\n",
            "Interval 650 (649000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.133 - mean_q: 0.176 - mean_eps: 0.100 - ale.lives: 2.076\n",
            "\n",
            "Interval 651 (650000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.009 - mae: 0.132 - mean_q: 0.174 - mean_eps: 0.100 - ale.lives: 1.903\n",
            "\n",
            "Interval 652 (651000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.337\n",
            "\n",
            "Interval 653 (652000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.132 - mean_q: 0.173 - mean_eps: 0.100 - ale.lives: 2.299\n",
            "\n",
            "Interval 654 (653000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 1.737\n",
            "\n",
            "Interval 655 (654000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.776\n",
            "\n",
            "Interval 656 (655000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 8.000 [2.000, 14.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 2.000\n",
            "\n",
            "Interval 657 (656000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0210\n",
            "Interval 658 (657000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 19.500 [13.000, 26.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.028\n",
            "\n",
            "Interval 659 (658000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.007 - mae: 0.128 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 1.818\n",
            "\n",
            "Interval 660 (659000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.007 - mae: 0.128 - mean_q: 0.169 - mean_eps: 0.100 - ale.lives: 2.391\n",
            "\n",
            "Interval 661 (660000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 7.500 [4.000, 11.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 2.003\n",
            "\n",
            "Interval 662 (661000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 10.500 [5.000, 16.000] - loss: 0.007 - mae: 0.127 - mean_q: 0.167 - mean_eps: 0.100 - ale.lives: 1.482\n",
            "\n",
            "Interval 663 (662000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0120\n",
            "Interval 664 (663000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.408\n",
            "\n",
            "Interval 665 (664000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0100\n",
            "2 episodes - episode_reward: 10.000 [6.000, 14.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.023\n",
            "\n",
            "Interval 666 (665000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.007 - mae: 0.126 - mean_q: 0.165 - mean_eps: 0.100 - ale.lives: 2.091\n",
            "\n",
            "Interval 667 (666000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 6.500 [4.000, 9.000] - loss: 0.008 - mae: 0.127 - mean_q: 0.166 - mean_eps: 0.100 - ale.lives: 2.098\n",
            "\n",
            "Interval 668 (667000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 0.128 - mean_q: 0.167 - mean_eps: 0.100 - ale.lives: 1.698\n",
            "\n",
            "Interval 669 (668000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.500 [11.000, 16.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 1.846\n",
            "\n",
            "Interval 670 (669000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.128\n",
            "\n",
            "Interval 671 (670000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 11.500 [8.000, 15.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.169 - mean_eps: 0.100 - ale.lives: 2.007\n",
            "\n",
            "Interval 672 (671000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.007 - mae: 0.127 - mean_q: 0.167 - mean_eps: 0.100 - ale.lives: 2.429\n",
            "\n",
            "Interval 673 (672000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "Interval 674 (673000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.289\n",
            "\n",
            "Interval 675 (674000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 20.000 [11.000, 29.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.913\n",
            "\n",
            "Interval 676 (675000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.006 - mae: 0.128 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 2.649\n",
            "\n",
            "Interval 677 (676000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0140\n",
            "Interval 678 (677000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 21.500 [16.000, 27.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.733\n",
            "\n",
            "Interval 679 (678000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.745\n",
            "\n",
            "Interval 680 (679000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 1.909\n",
            "\n",
            "Interval 681 (680000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 1.845\n",
            "\n",
            "Interval 682 (681000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 16.500 [6.000, 27.000] - loss: 0.007 - mae: 0.129 - mean_q: 0.169 - mean_eps: 0.100 - ale.lives: 1.978\n",
            "\n",
            "Interval 683 (682000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 2.157\n",
            "\n",
            "Interval 684 (683000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.007 - mae: 0.127 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 1.936\n",
            "\n",
            "Interval 685 (684000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.086\n",
            "\n",
            "Interval 686 (685000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.500 [4.000, 15.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 1.532\n",
            "\n",
            "Interval 687 (686000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 2.104\n",
            "\n",
            "Interval 688 (687000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 1.839\n",
            "\n",
            "Interval 689 (688000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.007 - mae: 0.128 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 1.572\n",
            "\n",
            "Interval 690 (689000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 1.473\n",
            "\n",
            "Interval 691 (690000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 15.500 [12.000, 19.000] - loss: 0.008 - mae: 0.128 - mean_q: 0.169 - mean_eps: 0.100 - ale.lives: 2.319\n",
            "\n",
            "Interval 692 (691000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 2.040\n",
            "\n",
            "Interval 693 (692000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.131 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.170\n",
            "\n",
            "Interval 694 (693000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.129 - mean_q: 0.168 - mean_eps: 0.100 - ale.lives: 2.605\n",
            "\n",
            "Interval 695 (694000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.130 - mean_q: 0.169 - mean_eps: 0.100 - ale.lives: 1.798\n",
            "\n",
            "Interval 696 (695000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.007 - mae: 0.128 - mean_q: 0.167 - mean_eps: 0.100 - ale.lives: 1.924\n",
            "\n",
            "Interval 697 (696000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.132 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 1.897\n",
            "\n",
            "Interval 698 (697000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 16.500 [10.000, 23.000] - loss: 0.007 - mae: 0.131 - mean_q: 0.170 - mean_eps: 0.100 - ale.lives: 1.966\n",
            "\n",
            "Interval 699 (698000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 12.500 [7.000, 18.000] - loss: 0.009 - mae: 0.133 - mean_q: 0.172 - mean_eps: 0.100 - ale.lives: 1.875\n",
            "\n",
            "Interval 700 (699000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 4.000 [4.000, 4.000] - loss: 0.009 - mae: 0.132 - mean_q: 0.171 - mean_eps: 0.100 - ale.lives: 2.174\n",
            "\n",
            "Interval 701 (700000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.151 - mean_q: 0.197 - mean_eps: 0.100 - ale.lives: 1.839\n",
            "\n",
            "Interval 702 (701000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 19.500 [18.000, 21.000] - loss: 0.009 - mae: 0.151 - mean_q: 0.197 - mean_eps: 0.100 - ale.lives: 2.281\n",
            "\n",
            "Interval 703 (702000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.152 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.167\n",
            "\n",
            "Interval 704 (703000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 13.500 [11.000, 16.000] - loss: 0.008 - mae: 0.151 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.296\n",
            "\n",
            "Interval 705 (704000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 1.691\n",
            "\n",
            "Interval 706 (705000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.052\n",
            "\n",
            "Interval 707 (706000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 19.500 [11.000, 28.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.262\n",
            "\n",
            "Interval 708 (707000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.154 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 1.862\n",
            "\n",
            "Interval 709 (708000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 12.500 [12.000, 13.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 1.930\n",
            "\n",
            "Interval 710 (709000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.151 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.153\n",
            "\n",
            "Interval 711 (710000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.153 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 1.922\n",
            "\n",
            "Interval 712 (711000 steps performed)\n",
            "1000/1000 [==============================] - 20s 19ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.500 [6.000, 21.000] - loss: 0.009 - mae: 0.153 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 2.025\n",
            "\n",
            "Interval 713 (712000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.151 - mean_q: 0.197 - mean_eps: 0.100 - ale.lives: 1.660\n",
            "\n",
            "Interval 714 (713000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "3 episodes - episode_reward: 5.667 [5.000, 7.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.278\n",
            "\n",
            "Interval 715 (714000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 1.351\n",
            "\n",
            "Interval 716 (715000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.007 - mae: 0.150 - mean_q: 0.196 - mean_eps: 0.100 - ale.lives: 1.913\n",
            "\n",
            "Interval 717 (716000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.384\n",
            "\n",
            "Interval 718 (717000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.009 - mae: 0.153 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.131\n",
            "\n",
            "Interval 719 (718000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.007 - mae: 0.150 - mean_q: 0.196 - mean_eps: 0.100 - ale.lives: 1.965\n",
            "\n",
            "Interval 720 (719000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 1.876\n",
            "\n",
            "Interval 721 (720000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 1.645\n",
            "\n",
            "Interval 722 (721000 steps performed)\n",
            "1000/1000 [==============================] - 19s 18ms/step - reward: 0.0200\n",
            "3 episodes - episode_reward: 11.000 [7.000, 16.000] - loss: 0.009 - mae: 0.155 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.184\n",
            "\n",
            "Interval 723 (722000 steps performed)\n",
            "1000/1000 [==============================] - 17s 17ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.155 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.340\n",
            "\n",
            "Interval 724 (723000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 15.000 [14.000, 16.000] - loss: 0.007 - mae: 0.149 - mean_q: 0.195 - mean_eps: 0.100 - ale.lives: 2.064\n",
            "\n",
            "Interval 725 (724000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "Interval 726 (725000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.154 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 1.877\n",
            "\n",
            "Interval 727 (726000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 1.830\n",
            "\n",
            "Interval 728 (727000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 17.500 [5.000, 30.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.070\n",
            "\n",
            "Interval 729 (728000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.366\n",
            "\n",
            "Interval 730 (729000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 2.013\n",
            "\n",
            "Interval 731 (730000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.154\n",
            "\n",
            "Interval 732 (731000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 1.635\n",
            "\n",
            "Interval 733 (732000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0260\n",
            "2 episodes - episode_reward: 16.000 [8.000, 24.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.010\n",
            "\n",
            "Interval 734 (733000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 13.500 [11.000, 16.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.076\n",
            "\n",
            "Interval 735 (734000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.209\n",
            "\n",
            "Interval 736 (735000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 1.815\n",
            "\n",
            "Interval 737 (736000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.417\n",
            "\n",
            "Interval 738 (737000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.009 - mae: 0.154 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.083\n",
            "\n",
            "Interval 739 (738000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 15.000 [9.000, 21.000] - loss: 0.009 - mae: 0.155 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 2.126\n",
            "\n",
            "Interval 740 (739000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 1.640\n",
            "\n",
            "Interval 741 (740000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "3 episodes - episode_reward: 11.333 [8.000, 15.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.253\n",
            "\n",
            "Interval 742 (741000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.152 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 2.189\n",
            "\n",
            "Interval 743 (742000 steps performed)\n",
            "1000/1000 [==============================] - 18s 17ms/step - reward: 0.0260\n",
            "Interval 744 (743000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 36.000 [36.000, 36.000] - loss: 0.010 - mae: 0.158 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 1.668\n",
            "\n",
            "Interval 745 (744000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 1.961\n",
            "\n",
            "Interval 746 (745000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.937\n",
            "\n",
            "Interval 747 (746000 steps performed)\n",
            "1000/1000 [==============================] - 18s 18ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.155 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 2.325\n",
            "\n",
            "Interval 748 (747000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.500 [11.000, 16.000] - loss: 0.010 - mae: 0.161 - mean_q: 0.207 - mean_eps: 0.100 - ale.lives: 1.723\n",
            "\n",
            "Interval 749 (748000 steps performed)\n",
            "1000/1000 [==============================] - 19s 19ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 10.000 [4.000, 16.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.008\n",
            "\n",
            "Interval 750 (749000 steps performed)\n",
            "1000/1000 [==============================] - 20s 20ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.000 [7.000, 9.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 1.833\n",
            "\n",
            "Interval 751 (750000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.170\n",
            "\n",
            "Interval 752 (751000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.155 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.372\n",
            "\n",
            "Interval 753 (752000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.155 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 2.233\n",
            "\n",
            "Interval 754 (753000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.155 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.041\n",
            "\n",
            "Interval 755 (754000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.153 - mean_q: 0.198 - mean_eps: 0.100 - ale.lives: 1.482\n",
            "\n",
            "Interval 756 (755000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 13.500 [13.000, 14.000] - loss: 0.008 - mae: 0.155 - mean_q: 0.201 - mean_eps: 0.100 - ale.lives: 1.845\n",
            "\n",
            "Interval 757 (756000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.159 - mean_q: 0.205 - mean_eps: 0.100 - ale.lives: 1.931\n",
            "\n",
            "Interval 758 (757000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.158 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 2.493\n",
            "\n",
            "Interval 759 (758000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.692\n",
            "\n",
            "Interval 760 (759000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0250\n",
            "2 episodes - episode_reward: 21.000 [18.000, 24.000] - loss: 0.008 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.325\n",
            "\n",
            "Interval 761 (760000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "Interval 762 (761000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 35.000 [35.000, 35.000] - loss: 0.008 - mae: 0.154 - mean_q: 0.199 - mean_eps: 0.100 - ale.lives: 2.088\n",
            "\n",
            "Interval 763 (762000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 10.500 [10.000, 11.000] - loss: 0.008 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 2.220\n",
            "\n",
            "Interval 764 (763000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.830\n",
            "\n",
            "Interval 765 (764000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.042\n",
            "\n",
            "Interval 766 (765000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.159 - mean_q: 0.206 - mean_eps: 0.100 - ale.lives: 1.508\n",
            "\n",
            "Interval 767 (766000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 2.711\n",
            "\n",
            "Interval 768 (767000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.270\n",
            "\n",
            "Interval 769 (768000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.010 - mae: 0.160 - mean_q: 0.206 - mean_eps: 0.100 - ale.lives: 1.755\n",
            "\n",
            "Interval 770 (769000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 0.009 - mae: 0.160 - mean_q: 0.207 - mean_eps: 0.100 - ale.lives: 2.143\n",
            "\n",
            "Interval 771 (770000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 2.154\n",
            "\n",
            "Interval 772 (771000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.159 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.941\n",
            "\n",
            "Interval 773 (772000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.156 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 2.547\n",
            "\n",
            "Interval 774 (773000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.306\n",
            "\n",
            "Interval 775 (774000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.009 - mae: 0.158 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 2.049\n",
            "\n",
            "Interval 776 (775000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 8.500 [7.000, 10.000] - loss: 0.007 - mae: 0.151 - mean_q: 0.196 - mean_eps: 0.100 - ale.lives: 1.956\n",
            "\n",
            "Interval 777 (776000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 1.879\n",
            "\n",
            "Interval 778 (777000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.009 - mae: 0.159 - mean_q: 0.206 - mean_eps: 0.100 - ale.lives: 2.190\n",
            "\n",
            "Interval 779 (778000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0300\n",
            "2 episodes - episode_reward: 21.500 [13.000, 30.000] - loss: 0.010 - mae: 0.161 - mean_q: 0.208 - mean_eps: 0.100 - ale.lives: 1.677\n",
            "\n",
            "Interval 780 (779000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.009 - mae: 0.159 - mean_q: 0.205 - mean_eps: 0.100 - ale.lives: 2.077\n",
            "\n",
            "Interval 781 (780000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.158 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 1.893\n",
            "\n",
            "Interval 782 (781000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 15.000 [6.000, 24.000] - loss: 0.010 - mae: 0.163 - mean_q: 0.209 - mean_eps: 0.100 - ale.lives: 2.017\n",
            "\n",
            "Interval 783 (782000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.500 [8.000, 17.000] - loss: 0.010 - mae: 0.160 - mean_q: 0.205 - mean_eps: 0.100 - ale.lives: 1.949\n",
            "\n",
            "Interval 784 (783000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 1.888\n",
            "\n",
            "Interval 785 (784000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 8.000 [5.000, 11.000] - loss: 0.008 - mae: 0.155 - mean_q: 0.200 - mean_eps: 0.100 - ale.lives: 2.203\n",
            "\n",
            "Interval 786 (785000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 12.000 [7.000, 17.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.729\n",
            "\n",
            "Interval 787 (786000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.898\n",
            "\n",
            "Interval 788 (787000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.972\n",
            "\n",
            "Interval 789 (788000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 0.009 - mae: 0.159 - mean_q: 0.207 - mean_eps: 0.100 - ale.lives: 2.032\n",
            "\n",
            "Interval 790 (789000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.161 - mean_q: 0.207 - mean_eps: 0.100 - ale.lives: 2.186\n",
            "\n",
            "Interval 791 (790000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "Interval 792 (791000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 17.500 [17.000, 18.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.066\n",
            "\n",
            "Interval 793 (792000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.157 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 2.029\n",
            "\n",
            "Interval 794 (793000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.160\n",
            "\n",
            "Interval 795 (794000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.158 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 1.656\n",
            "\n",
            "Interval 796 (795000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0280\n",
            "2 episodes - episode_reward: 13.000 [8.000, 18.000] - loss: 0.009 - mae: 0.158 - mean_q: 0.204 - mean_eps: 0.100 - ale.lives: 2.059\n",
            "\n",
            "Interval 797 (796000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "2 episodes - episode_reward: 11.000 [10.000, 12.000] - loss: 0.008 - mae: 0.155 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 2.227\n",
            "\n",
            "Interval 798 (797000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.157 - mean_q: 0.203 - mean_eps: 0.100 - ale.lives: 1.876\n",
            "\n",
            "Interval 799 (798000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.156 - mean_q: 0.202 - mean_eps: 0.100 - ale.lives: 1.988\n",
            "\n",
            "Interval 800 (799000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.500 [4.000, 13.000] - loss: 0.009 - mae: 0.159 - mean_q: 0.207 - mean_eps: 0.100 - ale.lives: 2.152\n",
            "\n",
            "Interval 801 (800000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 1.964\n",
            "\n",
            "Interval 802 (801000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0250\n",
            "2 episodes - episode_reward: 16.000 [15.000, 17.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.918\n",
            "\n",
            "Interval 803 (802000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 5.500 [5.000, 6.000] - loss: 0.008 - mae: 0.164 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.288\n",
            "\n",
            "Interval 804 (803000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.598\n",
            "\n",
            "Interval 805 (804000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 14.000 [11.000, 17.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 1.741\n",
            "\n",
            "Interval 806 (805000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.217 - mean_eps: 0.100 - ale.lives: 2.215\n",
            "\n",
            "Interval 807 (806000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 1.612\n",
            "\n",
            "Interval 808 (807000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 10.500 [9.000, 12.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 1.994\n",
            "\n",
            "Interval 809 (808000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 5.000 [3.000, 7.000] - loss: 0.008 - mae: 0.161 - mean_q: 0.211 - mean_eps: 0.100 - ale.lives: 1.965\n",
            "\n",
            "Interval 810 (809000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 1.843\n",
            "\n",
            "Interval 811 (810000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0250\n",
            "2 episodes - episode_reward: 17.500 [13.000, 22.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.800\n",
            "\n",
            "Interval 812 (811000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 11.000 [7.000, 15.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.934\n",
            "\n",
            "Interval 813 (812000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.523\n",
            "\n",
            "Interval 814 (813000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.164 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.555\n",
            "\n",
            "Interval 815 (814000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 3.000 [3.000, 3.000] - loss: 0.008 - mae: 0.166 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 2.040\n",
            "\n",
            "Interval 816 (815000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 13.500 [6.000, 21.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.003\n",
            "\n",
            "Interval 817 (816000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.698\n",
            "\n",
            "Interval 818 (817000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 10.000 [5.000, 15.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.217 - mean_eps: 0.100 - ale.lives: 2.125\n",
            "\n",
            "Interval 819 (818000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 12.000 [7.000, 17.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.038\n",
            "\n",
            "Interval 820 (819000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0100\n",
            "Interval 821 (820000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0270\n",
            "2 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.010 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.763\n",
            "\n",
            "Interval 822 (821000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0280\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.009 - mae: 0.163 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 2.166\n",
            "\n",
            "Interval 823 (822000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.853\n",
            "\n",
            "Interval 824 (823000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.862\n",
            "\n",
            "Interval 825 (824000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "3 episodes - episode_reward: 8.000 [3.000, 12.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 2.035\n",
            "\n",
            "Interval 826 (825000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 2.376\n",
            "\n",
            "Interval 827 (826000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.322\n",
            "\n",
            "Interval 828 (827000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.559\n",
            "\n",
            "Interval 829 (828000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.010 - mae: 0.170 - mean_q: 0.219 - mean_eps: 0.100 - ale.lives: 2.213\n",
            "\n",
            "Interval 830 (829000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.009 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 1.752\n",
            "\n",
            "Interval 831 (830000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.217 - mean_eps: 0.100 - ale.lives: 2.079\n",
            "\n",
            "Interval 832 (831000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.163 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 1.986\n",
            "\n",
            "Interval 833 (832000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "3 episodes - episode_reward: 8.333 [5.000, 13.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.206\n",
            "\n",
            "Interval 834 (833000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.413\n",
            "\n",
            "Interval 835 (834000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 12.500 [9.000, 16.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.044\n",
            "\n",
            "Interval 836 (835000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.820\n",
            "\n",
            "Interval 837 (836000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 13.500 [13.000, 14.000] - loss: 0.010 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.909\n",
            "\n",
            "Interval 838 (837000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.169 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 1.695\n",
            "\n",
            "Interval 839 (838000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 15.000 [13.000, 17.000] - loss: 0.010 - mae: 0.169 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 2.061\n",
            "\n",
            "Interval 840 (839000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.163 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 2.112\n",
            "\n",
            "Interval 841 (840000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0100\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.008 - mae: 0.162 - mean_q: 0.211 - mean_eps: 0.100 - ale.lives: 2.129\n",
            "\n",
            "Interval 842 (841000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.628\n",
            "\n",
            "Interval 843 (842000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.286\n",
            "\n",
            "Interval 844 (843000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 13.000 [9.000, 17.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.945\n",
            "\n",
            "Interval 845 (844000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 1.885\n",
            "\n",
            "Interval 846 (845000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0080\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 1.802\n",
            "\n",
            "Interval 847 (846000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 9.000 [4.000, 14.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.037\n",
            "\n",
            "Interval 848 (847000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 11.500 [6.000, 17.000] - loss: 0.010 - mae: 0.170 - mean_q: 0.219 - mean_eps: 0.100 - ale.lives: 2.405\n",
            "\n",
            "Interval 849 (848000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0140\n",
            "Interval 850 (849000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 14.000 [13.000, 15.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.155\n",
            "\n",
            "Interval 851 (850000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 14.000 [11.000, 17.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.481\n",
            "\n",
            "Interval 852 (851000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.169 - mean_q: 0.217 - mean_eps: 0.100 - ale.lives: 1.897\n",
            "\n",
            "Interval 853 (852000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.907\n",
            "\n",
            "Interval 854 (853000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.104\n",
            "\n",
            "Interval 855 (854000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0310\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.010 - mae: 0.171 - mean_q: 0.220 - mean_eps: 0.100 - ale.lives: 1.640\n",
            "\n",
            "Interval 856 (855000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.010 - mae: 0.170 - mean_q: 0.219 - mean_eps: 0.100 - ale.lives: 1.500\n",
            "\n",
            "Interval 857 (856000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 2.266\n",
            "\n",
            "Interval 858 (857000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 27.000 [27.000, 27.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 2.152\n",
            "\n",
            "Interval 859 (858000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0120\n",
            "2 episodes - episode_reward: 15.500 [6.000, 25.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.698\n",
            "\n",
            "Interval 860 (859000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.839\n",
            "\n",
            "Interval 861 (860000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0090\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.010 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.482\n",
            "\n",
            "Interval 862 (861000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0110\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.537\n",
            "\n",
            "Interval 863 (862000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 9.500 [9.000, 10.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 2.180\n",
            "\n",
            "Interval 864 (863000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.010 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.105\n",
            "\n",
            "Interval 865 (864000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "Interval 866 (865000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 15.500 [8.000, 23.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.080\n",
            "\n",
            "Interval 867 (866000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.008 - mae: 0.160 - mean_q: 0.208 - mean_eps: 0.100 - ale.lives: 2.109\n",
            "\n",
            "Interval 868 (867000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.211 - mean_eps: 0.100 - ale.lives: 1.960\n",
            "\n",
            "Interval 869 (868000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0110\n",
            "Interval 870 (869000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 15.500 [9.000, 22.000] - loss: 0.008 - mae: 0.164 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 1.747\n",
            "\n",
            "Interval 871 (870000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 12.500 [10.000, 15.000] - loss: 0.007 - mae: 0.162 - mean_q: 0.210 - mean_eps: 0.100 - ale.lives: 1.748\n",
            "\n",
            "Interval 872 (871000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 11.000 [5.000, 17.000] - loss: 0.010 - mae: 0.169 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 2.031\n",
            "\n",
            "Interval 873 (872000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.297\n",
            "\n",
            "Interval 874 (873000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0110\n",
            "3 episodes - episode_reward: 4.667 [4.000, 6.000] - loss: 0.008 - mae: 0.161 - mean_q: 0.210 - mean_eps: 0.100 - ale.lives: 2.155\n",
            "\n",
            "Interval 875 (874000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.918\n",
            "\n",
            "Interval 876 (875000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 14.000 [13.000, 15.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.856\n",
            "\n",
            "Interval 877 (876000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.162 - mean_q: 0.209 - mean_eps: 0.100 - ale.lives: 2.028\n",
            "\n",
            "Interval 878 (877000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 1.954\n",
            "\n",
            "Interval 879 (878000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.495\n",
            "\n",
            "Interval 880 (879000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0270\n",
            "2 episodes - episode_reward: 17.000 [16.000, 18.000] - loss: 0.009 - mae: 0.164 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.274\n",
            "\n",
            "Interval 881 (880000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 1.794\n",
            "\n",
            "Interval 882 (881000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.164 - mean_q: 0.210 - mean_eps: 0.100 - ale.lives: 2.013\n",
            "\n",
            "Interval 883 (882000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.008 - mae: 0.163 - mean_q: 0.210 - mean_eps: 0.100 - ale.lives: 1.830\n",
            "\n",
            "Interval 884 (883000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 11.000 [5.000, 17.000] - loss: 0.008 - mae: 0.163 - mean_q: 0.211 - mean_eps: 0.100 - ale.lives: 1.984\n",
            "\n",
            "Interval 885 (884000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 1.852\n",
            "\n",
            "Interval 886 (885000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.196\n",
            "\n",
            "Interval 887 (886000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.122\n",
            "\n",
            "Interval 888 (887000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 14.500 [7.000, 22.000] - loss: 0.008 - mae: 0.164 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 2.126\n",
            "\n",
            "Interval 889 (888000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.168 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.854\n",
            "\n",
            "Interval 890 (889000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.008 - mae: 0.166 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 1.756\n",
            "\n",
            "Interval 891 (890000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 14.000 [10.000, 18.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 2.049\n",
            "\n",
            "Interval 892 (891000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 2.184\n",
            "\n",
            "Interval 893 (892000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.008 - mae: 0.167 - mean_q: 0.214 - mean_eps: 0.100 - ale.lives: 2.019\n",
            "\n",
            "Interval 894 (893000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.165 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 1.677\n",
            "\n",
            "Interval 895 (894000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.009 - mae: 0.166 - mean_q: 0.213 - mean_eps: 0.100 - ale.lives: 2.549\n",
            "\n",
            "Interval 896 (895000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.169 - mean_q: 0.217 - mean_eps: 0.100 - ale.lives: 1.588\n",
            "\n",
            "Interval 897 (896000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 12.500 [6.000, 19.000] - loss: 0.009 - mae: 0.168 - mean_q: 0.216 - mean_eps: 0.100 - ale.lives: 1.856\n",
            "\n",
            "Interval 898 (897000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.009 - mae: 0.170 - mean_q: 0.218 - mean_eps: 0.100 - ale.lives: 2.122\n",
            "\n",
            "Interval 899 (898000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.167 - mean_q: 0.215 - mean_eps: 0.100 - ale.lives: 1.668\n",
            "\n",
            "Interval 900 (899000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.164 - mean_q: 0.212 - mean_eps: 0.100 - ale.lives: 2.012\n",
            "\n",
            "Interval 901 (900000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.000 [12.000, 18.000] - loss: 0.009 - mae: 0.177 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 2.551\n",
            "\n",
            "Interval 902 (901000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 2.126\n",
            "\n",
            "Interval 903 (902000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.009 - mae: 0.177 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 2.179\n",
            "\n",
            "Interval 904 (903000 steps performed)\n",
            "1000/1000 [==============================] - 21s 21ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.010 - mae: 0.180 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 1.939\n",
            "\n",
            "Interval 905 (904000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.008 - mae: 0.177 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 2.064\n",
            "\n",
            "Interval 906 (905000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 15.000 [9.000, 21.000] - loss: 0.010 - mae: 0.180 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 1.866\n",
            "\n",
            "Interval 907 (906000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.175 - mean_q: 0.225 - mean_eps: 0.100 - ale.lives: 2.606\n",
            "\n",
            "Interval 908 (907000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 12.500 [12.000, 13.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 1.698\n",
            "\n",
            "Interval 909 (908000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 7.500 [6.000, 9.000] - loss: 0.007 - mae: 0.175 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 2.317\n",
            "\n",
            "Interval 910 (909000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 1.705\n",
            "\n",
            "Interval 911 (910000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 2.078\n",
            "\n",
            "Interval 912 (911000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0230\n",
            "Interval 913 (912000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0120\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.008 - mae: 0.177 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 1.983\n",
            "\n",
            "Interval 914 (913000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 14.500 [13.000, 16.000] - loss: 0.009 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 1.979\n",
            "\n",
            "Interval 915 (914000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.175 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 2.089\n",
            "\n",
            "Interval 916 (915000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.009 - mae: 0.179 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 2.320\n",
            "\n",
            "Interval 917 (916000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.182 - mean_q: 0.234 - mean_eps: 0.100 - ale.lives: 2.372\n",
            "\n",
            "Interval 918 (917000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.176 - mean_q: 0.225 - mean_eps: 0.100 - ale.lives: 2.133\n",
            "\n",
            "Interval 919 (918000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 1.914\n",
            "\n",
            "Interval 920 (919000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 6.500 [6.000, 7.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.222\n",
            "\n",
            "Interval 921 (920000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 7.000 [7.000, 7.000] - loss: 0.009 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.730\n",
            "\n",
            "Interval 922 (921000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0270\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 2.346\n",
            "\n",
            "Interval 923 (922000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 36.000 [36.000, 36.000] - loss: 0.009 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.147\n",
            "\n",
            "Interval 924 (923000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.115\n",
            "\n",
            "Interval 925 (924000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.008 - mae: 0.175 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 1.729\n",
            "\n",
            "Interval 926 (925000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 13.500 [5.000, 22.000] - loss: 0.007 - mae: 0.173 - mean_q: 0.223 - mean_eps: 0.100 - ale.lives: 1.647\n",
            "\n",
            "Interval 927 (926000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.008 - mae: 0.174 - mean_q: 0.224 - mean_eps: 0.100 - ale.lives: 1.788\n",
            "\n",
            "Interval 928 (927000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 13.000 [10.000, 16.000] - loss: 0.009 - mae: 0.179 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 1.773\n",
            "\n",
            "Interval 929 (928000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0250\n",
            "1 episodes - episode_reward: 22.000 [22.000, 22.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.047\n",
            "\n",
            "Interval 930 (929000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 1.830\n",
            "\n",
            "Interval 931 (930000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 13.500 [10.000, 17.000] - loss: 0.009 - mae: 0.179 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 2.270\n",
            "\n",
            "Interval 932 (931000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0110\n",
            "2 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.125\n",
            "\n",
            "Interval 933 (932000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 13.000 [13.000, 13.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.889\n",
            "\n",
            "Interval 934 (933000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 11.500 [5.000, 18.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.104\n",
            "\n",
            "Interval 935 (934000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.007 - mae: 0.177 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 1.894\n",
            "\n",
            "Interval 936 (935000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 14.500 [9.000, 20.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.015\n",
            "\n",
            "Interval 937 (936000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 10.000 [10.000, 10.000] - loss: 0.008 - mae: 0.176 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 1.468\n",
            "\n",
            "Interval 938 (937000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.019\n",
            "\n",
            "Interval 939 (938000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0250\n",
            "2 episodes - episode_reward: 15.500 [14.000, 17.000] - loss: 0.009 - mae: 0.177 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.936\n",
            "\n",
            "Interval 940 (939000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0120\n",
            "Interval 941 (940000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 17.000 [14.000, 20.000] - loss: 0.009 - mae: 0.176 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 2.111\n",
            "\n",
            "Interval 942 (941000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.009 - mae: 0.177 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 1.947\n",
            "\n",
            "Interval 943 (942000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 24.000 [24.000, 24.000] - loss: 0.008 - mae: 0.176 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 2.000\n",
            "\n",
            "Interval 944 (943000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 1.969\n",
            "\n",
            "Interval 945 (944000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 8.000 [8.000, 8.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.420\n",
            "\n",
            "Interval 946 (945000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 17.500 [15.000, 20.000] - loss: 0.008 - mae: 0.176 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 2.041\n",
            "\n",
            "Interval 947 (946000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "Interval 948 (947000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 17.000 [8.000, 26.000] - loss: 0.009 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 1.964\n",
            "\n",
            "Interval 949 (948000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0290\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.139\n",
            "\n",
            "Interval 950 (949000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.007 - mae: 0.176 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 1.677\n",
            "\n",
            "Interval 951 (950000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 15.000 [9.000, 21.000] - loss: 0.010 - mae: 0.182 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 2.360\n",
            "\n",
            "Interval 952 (951000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 11.000 [5.000, 17.000] - loss: 0.009 - mae: 0.177 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 2.014\n",
            "\n",
            "Interval 953 (952000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.009 - mae: 0.178 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.735\n",
            "\n",
            "Interval 954 (953000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0160\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.179 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.288\n",
            "\n",
            "Interval 955 (954000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.176 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 1.785\n",
            "\n",
            "Interval 956 (955000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "2 episodes - episode_reward: 18.500 [16.000, 21.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.892\n",
            "\n",
            "Interval 957 (956000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.176 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 2.091\n",
            "\n",
            "Interval 958 (957000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0250\n",
            "Interval 959 (958000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 19.500 [10.000, 29.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 2.194\n",
            "\n",
            "Interval 960 (959000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0180\n",
            "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 1.854\n",
            "\n",
            "Interval 961 (960000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.298\n",
            "\n",
            "Interval 962 (961000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 11.000 [7.000, 15.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.061\n",
            "\n",
            "Interval 963 (962000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0140\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.009 - mae: 0.183 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 1.785\n",
            "\n",
            "Interval 964 (963000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "2 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.096\n",
            "\n",
            "Interval 965 (964000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.007 - mae: 0.176 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 2.159\n",
            "\n",
            "Interval 966 (965000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0140\n",
            "2 episodes - episode_reward: 10.000 [8.000, 12.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 2.204\n",
            "\n",
            "Interval 967 (966000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 12.000 [12.000, 12.000] - loss: 0.007 - mae: 0.178 - mean_q: 0.227 - mean_eps: 0.100 - ale.lives: 1.839\n",
            "\n",
            "Interval 968 (967000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.454\n",
            "\n",
            "Interval 969 (968000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.182 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 1.387\n",
            "\n",
            "Interval 970 (969000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0260\n",
            "2 episodes - episode_reward: 15.000 [14.000, 16.000] - loss: 0.009 - mae: 0.183 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 2.321\n",
            "\n",
            "Interval 971 (970000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.007 - mae: 0.179 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 2.277\n",
            "\n",
            "Interval 972 (971000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.189\n",
            "\n",
            "Interval 973 (972000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0240\n",
            "2 episodes - episode_reward: 15.500 [10.000, 21.000] - loss: 0.010 - mae: 0.181 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.133\n",
            "\n",
            "Interval 974 (973000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 11.000 [8.000, 14.000] - loss: 0.009 - mae: 0.184 - mean_q: 0.234 - mean_eps: 0.100 - ale.lives: 1.966\n",
            "\n",
            "Interval 975 (974000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.177 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 1.849\n",
            "\n",
            "Interval 976 (975000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0230\n",
            "1 episodes - episode_reward: 23.000 [23.000, 23.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 1.986\n",
            "\n",
            "Interval 977 (976000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.182 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 1.902\n",
            "\n",
            "Interval 978 (977000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0160\n",
            "2 episodes - episode_reward: 12.000 [6.000, 18.000] - loss: 0.008 - mae: 0.180 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 2.028\n",
            "\n",
            "Interval 979 (978000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 6.000 [6.000, 6.000] - loss: 0.009 - mae: 0.180 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 2.165\n",
            "\n",
            "Interval 980 (979000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.184 - mean_q: 0.234 - mean_eps: 0.100 - ale.lives: 2.413\n",
            "\n",
            "Interval 981 (980000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0150\n",
            "1 episodes - episode_reward: 25.000 [25.000, 25.000] - loss: 0.010 - mae: 0.183 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 2.184\n",
            "\n",
            "Interval 982 (981000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.229 - mean_eps: 0.100 - ale.lives: 2.198\n",
            "\n",
            "Interval 983 (982000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 5.000 [5.000, 5.000] - loss: 0.009 - mae: 0.182 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.378\n",
            "\n",
            "Interval 984 (983000 steps performed)\n",
            "1000/1000 [==============================] - 23s 22ms/step - reward: 0.0260\n",
            "1 episodes - episode_reward: 26.000 [26.000, 26.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.117\n",
            "\n",
            "Interval 985 (984000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 21.000 [21.000, 21.000] - loss: 0.010 - mae: 0.182 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 2.146\n",
            "\n",
            "Interval 986 (985000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 28.000 [28.000, 28.000] - loss: 0.008 - mae: 0.180 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 1.553\n",
            "\n",
            "Interval 987 (986000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0130\n",
            "2 episodes - episode_reward: 9.500 [3.000, 16.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 1.994\n",
            "\n",
            "Interval 988 (987000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 14.000 [14.000, 14.000] - loss: 0.010 - mae: 0.187 - mean_q: 0.237 - mean_eps: 0.100 - ale.lives: 2.133\n",
            "\n",
            "Interval 989 (988000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0170\n",
            "2 episodes - episode_reward: 13.000 [11.000, 15.000] - loss: 0.009 - mae: 0.182 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 2.229\n",
            "\n",
            "Interval 990 (989000 steps performed)\n",
            "1000/1000 [==============================] - 7223s 7s/step - reward: 0.0170\n",
            "1 episodes - episode_reward: 9.000 [9.000, 9.000] - loss: 0.008 - mae: 0.180 - mean_q: 0.231 - mean_eps: 0.100 - ale.lives: 1.697\n",
            "\n",
            "Interval 991 (990000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0210\n",
            "2 episodes - episode_reward: 12.000 [11.000, 13.000] - loss: 0.008 - mae: 0.177 - mean_q: 0.226 - mean_eps: 0.100 - ale.lives: 1.702\n",
            "\n",
            "Interval 992 (991000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 19.000 [19.000, 19.000] - loss: 0.009 - mae: 0.183 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 1.913\n",
            "\n",
            "Interval 993 (992000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0210\n",
            "1 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.447\n",
            "\n",
            "Interval 994 (993000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0180\n",
            "1 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.009 - mae: 0.183 - mean_q: 0.233 - mean_eps: 0.100 - ale.lives: 1.412\n",
            "\n",
            "Interval 995 (994000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0200\n",
            "1 episodes - episode_reward: 15.000 [15.000, 15.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 2.054\n",
            "\n",
            "Interval 996 (995000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0150\n",
            "2 episodes - episode_reward: 14.000 [7.000, 21.000] - loss: 0.009 - mae: 0.182 - mean_q: 0.232 - mean_eps: 0.100 - ale.lives: 1.964\n",
            "\n",
            "Interval 997 (996000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0220\n",
            "1 episodes - episode_reward: 20.000 [20.000, 20.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 1.891\n",
            "\n",
            "Interval 998 (997000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0190\n",
            "1 episodes - episode_reward: 17.000 [17.000, 17.000] - loss: 0.008 - mae: 0.179 - mean_q: 0.228 - mean_eps: 0.100 - ale.lives: 1.710\n",
            "\n",
            "Interval 999 (998000 steps performed)\n",
            "1000/1000 [==============================] - 22s 22ms/step - reward: 0.0200\n",
            "2 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 0.009 - mae: 0.181 - mean_q: 0.230 - mean_eps: 0.100 - ale.lives: 1.823\n",
            "\n",
            "Interval 1000 (999000 steps performed)\n",
            "1000/1000 [==============================] - 23s 23ms/step - reward: 0.0180\n",
            "done, took 18323.038 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "weights_filename = 'dqn5_{}_weights.h5f'.format(env_name)\n",
        "checkpoint_weights_filename = 'dqn5_' + env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn5_{}_log.json'.format(env_name)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "dqn.fit(\n",
        "    env,\n",
        "    callbacks=callbacks,\n",
        "    nb_steps=1000000,\n",
        "    log_interval=1000,\n",
        "    visualize=False\n",
        ")\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OHYryKd1Gb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 28.000, steps: 1024\n",
            "Episode 2: reward: 19.000, steps: 590\n",
            "Episode 3: reward: 27.000, steps: 854\n",
            "Episode 4: reward: 22.000, steps: 830\n",
            "Episode 5: reward: 12.000, steps: 480\n",
            "Episode 6: reward: 32.000, steps: 1313\n",
            "Episode 7: reward: 17.000, steps: 604\n",
            "Episode 8: reward: 31.000, steps: 1157\n",
            "Episode 9: reward: 33.000, steps: 1089\n",
            "Episode 10: reward: 23.000, steps: 839\n"
          ]
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn5_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "results = dqn.test(env, nb_episodes=10, visualize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward: 24.4\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average reward: {np.mean(results.history['episode_reward']):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cronología"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Primeros pasos\n",
        "\n",
        "Iniciamos el proyecto práctico utilizando como base la sesión práctica 5 de la asignatura, a través del notebook por defecto de la sesión. Creamos nuestro propio entorno virtual en local y ejecutamos el modelo con una red neuronal similar al notebook de la sesión práctica. El promedio de las recompensas obtenido oscilaba entre los 14 y 15 puntos de media, lo que nos llevó a iniciar una serie de cambios para mejorar estos resultados.\n",
        "\n",
        "### Modificaciones iniciales\n",
        "\n",
        "Las primeras modificaciones consistieron en aumentar los pasos de entrenamiento y cambiar el 'learning rate', acorde a lo aprendido en la asignatura de Redes Neuronales. Aumentar los pasos de entrenamiento nos permitió proporcionar más datos y tiempo al modelo para aprender, mientras que ajustar la tasa de aprendizaje nos ayudó a controlar la velocidad a la que el modelo se ajustaba a los datos. Sin embargo, estos ajustes iniciales no fueron suficientes, por lo que decidimos hacer más compleja la arquitectura de la red neuronal.\n",
        "\n",
        "### Primera modificación de la red neuronal\n",
        "\n",
        "Empezamos añadiendo capas convolucionales, incrementando la profundidad del modelo y añadiendo normalizaciones. De esta manera, buscábamos poder recabar más información de cada una de las imágenes que fuesen procesadas por la red neuronal, procedentes de las capturas de pantalla del videojuego.\n",
        "\n",
        "| Layer (type)                               | Output Shape       | Param # |\n",
        "|--------------------------------------------|--------------------|---------|\n",
        "| permute_1 (Permute)                        | (None, 84, 84, 4)  | 0       |\n",
        "| conv2d_3 (Conv2D)                          | (None, 21, 21, 32) | 8224    |\n",
        "| activation_5 (Activation)                  | (None, 21, 21, 32) | 0       |\n",
        "| batch_normalization_4 (BatchNormalization) | (None, 21, 21, 32) | 128     |\n",
        "| dropout_4 (Dropout)                        | (None, 21, 21, 32) | 0       |\n",
        "| conv2d_4 (Conv2D)                          | (None, 11, 11, 64) | 32832   |\n",
        "| activation_6 (Activation)                  | (None, 11, 11, 64) | 0       |\n",
        "| batch_normalization_5 (BatchNormalization) | (None, 11, 11, 64) | 256     |\n",
        "| dropout_5 (Dropout)                        | (None, 11, 11, 64) | 0       |\n",
        "| conv2d_5 (Conv2D)                          | (None, 11, 11, 64) | 36928   |\n",
        "| activation_7 (Activation)                  | (None, 11, 11, 64) | 0       |\n",
        "| batch_normalization_6 (BatchNormalization) | (None, 11, 11, 64) | 256     |\n",
        "| dropout_6 (Dropout)                        | (None, 11, 11, 64) | 0       |\n",
        "| flatten_1 (Flatten)                        | (None, 7744)       | 0       |\n",
        "| dense_2 (Dense)                            | (None, 512)        | 3965440 |\n",
        "| activation_8 (Activation)                  | (None, 512)        | 0       |\n",
        "| batch_normalization_7 (BatchNormalization) | (None, 512)        | 2048    |\n",
        "| dropout_7 (Dropout)                        | (None, 512)        | 0       |\n",
        "| dense_3 (Dense)                            | (None, 6)          | 3078    |\n",
        "| activation_9 (Activation)                  | (None, 6)          | 0       |\n",
        "\n",
        "**Total params:** 4,049,190  \n",
        "**Trainable params:** 4,047,846  \n",
        "**Non-trainable params:** 1,344\n",
        "\n",
        "Expandimos la arquitectura a tres capas convolucionales, con la intención de mejorar aún más la capacidad del modelo para aprender representaciones significativas. Las normalizaciones se añadieron para evitar el sobreentrenamiento y que la red aprendiera de una manera más uniforme.\n",
        "\n",
        "En cuanto al agente, la configuración se mantuvo con respecto a los intentos anteriores. Al no obtener mejoras significativas, decidimos aumentar los steps a 1750000, pero los resultados no tuvieron la mejora que se esperaba, siendo la media inferior a 17.\n",
        "\n",
        "![Media de recompensas con 10 episodios de 16.9](Images/Score_17.png)\n",
        "\n",
        "### Tratamiento de la 'policy'\n",
        "\n",
        "El siguiente parámetro que decidimos cambiar y utilizar fue la 'policy'. Teniendo en cuenta que el agente es invariable en nuestra práctica (DQN), probamos con 'Boltzmann' y 'DecayEpsGreedy'. Sin embargo, los resultados no fueron significativos, llegando a empeorar los resultados previos a las pruebas.\n",
        "\n",
        "### Memoria y 'steps_warmup'\n",
        "\n",
        "A continuación, aumentamos el parámetro del agente DQN `steps_warmup` y el límite de la memoria de experiencia. La intención de estos cambios es aumentar el almacenamiento con la que el modelo guarda las experiencias vividas durante la exploración. Es por ello, que el aumento de 'steps_warmup' tiene sentido para que el modelo explore y aprenda de situaciones diferentes, obteniendo así una base de conocimiento sobre la que entrenará en los siguientes pasos. Esta estrategia de aumento de la memoria y de los pasos iniciales de exploración, es ideal para problemas como los que nos enfrentamos en el proyecto, donde el juego tiene cierta complejidad y necesita aprender del entorno.\n",
        "\n",
        "Este cambio asumido por el grupo en el proyecto, permitió un ligero aumento de los resultados obtenidos por el modelo. Ahora sí, conseguimos alcanzar el mínimo de 20 puntos de recompensa de media para 10 partidas. Sin embargo, el modelo no siempre obtiene esa media deseada, puesto que la varianza de los resultados es ciertamente grande y el modelo se entrena con demasiados parámetros (un entrenamiento cualquiera podía llegar a muchas horas sin obtener mejoras significativas por la red neuronal).\n",
        "\n",
        "![Media de recompensas con 10 episodios de 21.4](Images/Score_18.png)\n",
        "\n",
        "![Media de recompensas con 10 episodios de 21.4](Images/Score_21.png)\n",
        "\n",
        "Como caso curioso durante el entrenamiento y pruebas de distintos parámetros, se decidió entrenar un agente con poca memoria y un `steps_warmup` para la exploración bastante bajos. Para nuestra sorpresa, obtuvimos un modelo que proporcionaba buenos resultados en los tests de evaluación. Sin embargo, al observar las partidas jugadas, se aprecia que el agente no se mueve prácticamente de su posición, dedicándose exclusivamente a disparar.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"Images/Juego_1.png\" alt=\"Imagen del juego 1\" />\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"Images/Juego_2.png\" alt=\"Imagen del juego 2\" />\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"Images/Juego_3.png\" alt=\"Imagen del juego 3\" />\n",
        "</p>\n",
        "\n",
        "![Media de recompensas con 10 episodios de 20.0](Images/Score_20.png)\n",
        "\n",
        "La conclusión que obtenemos en este caso, es que unos buenos resultados de recompensa no son motivo para justificar que un modelo sea realmente bueno. Lo que sucedía en estas partidas fue que el modelo conseguía eliminar a gran parte de los enemigos, suponiendo un valor de recompensa alto, pero todos esos enemigos se encontraban en la zona izquierda de la pantalla porque el agente no se movía de su posición. Además, todas las partidas eran prácticamente iguales, por lo que nuestro agente no es capaz de realizar distintas estrategias para abordar el mismo problema.\n",
        "\n",
        "### Simplificación de la red\n",
        "\n",
        "En base a la explicación anterior, decidimos simplificar la red neuronal con la idea de optimizar el proceso de entrenamiento y quizás, obtener mejores resultados que los anteriores.\n",
        "\n",
        "| Layer (type)                 | Output Shape         | Param #   |\n",
        "|------------------------------|----------------------|-----------|\n",
        "| permute (Permute)            | (None, 84, 84, 4)    | 0         |\n",
        "| conv2d (Conv2D)              | (None, 20, 20, 32)   | 8224      |\n",
        "| activation (Activation)      | (None, 20, 20, 32)   | 0         |\n",
        "| conv2d_1 (Conv2D)            | (None, 9, 9, 64)     | 32832     |\n",
        "| activation_1 (Activation)    | (None, 9, 9, 64)     | 0         |\n",
        "| conv2d_2 (Conv2D)            | (None, 7, 7, 64)     | 36928     |\n",
        "| activation_2 (Activation)    | (None, 7, 7, 64)     | 0         |\n",
        "| flatten (Flatten)            | (None, 3136)         | 0         |\n",
        "| dense (Dense)                | (None, 512)          | 1606144   |\n",
        "| activation_3 (Activation)    | (None, 512)          | 0         |\n",
        "| dense_1 (Dense)              | (None, 256)          | 131328    |\n",
        "| activation_4 (Activation)    | (None, 256)          | 0         |\n",
        "| dense_2 (Dense)              | (None, 6)            | 1542      |\n",
        "| activation_5 (Activation)    | (None, 6)            | 0         |\n",
        "\n",
        "**Total params:** 1,816,998  \n",
        "**Trainable params:** 1,816,998  \n",
        "**Non-trainable params:** 0\n",
        "\n",
        "Este cambio, supuso una reducción de los parámetros entrenables en más de un 50%. Lo más importante, los resultados medios de recompensa seguían siendo prácticamente iguales a los de la red neuronal anterior.\n",
        "\n",
        "### Resultados finales:\n",
        "\n",
        "Finalmente, se decidió conservar la estrategia de los parámetros del agente DQN, utilizar `LinearAnnealedPolicy` como 'policy' y utilizar la red neuronal optimizada. De esta manera, y con una estrategia más o menos acertada, entrenamos el modelo por un gran número de pasos, con la idea de mejorar los resultados lo máximo posible. Se utilizaron 1 millón de pasos de entrenamiento (como se puede ver en los apartados anteriores de este notebook), con una duración de entrenamiento de más de 7 horas.\n",
        "\n",
        "| **Episode** | **Reward** | **Steps** |\n",
        "|-------------|------------|-----------|\n",
        "| Episode 1   | 28.000     | 1024      |\n",
        "| Episode 2   | 19.000     | 590       |\n",
        "| Episode 3   | 27.000     | 854       |\n",
        "| Episode 4   | 22.000     | 830       |\n",
        "| Episode 5   | 12.000     | 480       |\n",
        "| Episode 6   | 32.000     | 1313      |\n",
        "| Episode 7   | 17.000     | 604       |\n",
        "| Episode 8   | 31.000     | 1157      |\n",
        "| Episode 9   | 33.000     | 1089      |\n",
        "| Episode 10  | 23.000     | 839       |\n",
        "\n",
        "=======================\n",
        "\n",
        "**Average reward: 24.4**\n",
        "\n",
        "La recompensa media llegó a ser de 24.4. Además, obtuvimos resultados de partidas con un 'reward' de 31 y 33. En esas partidas en concreto, el modelo casi consigue finalizar la partida al completo eliminando a casi todas las naves rivales.\n",
        "\n",
        "![Media de recompensas con 10 episodios de 24.4](Images/Score_24.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parámetros controlables durante el proyecto\n",
        "\n",
        "En este apartado, queremos explicar con más detenimiento los parámetros que hemos ido probando a lo largo de la práctica y cómo pueden afectar al rendimiento del modelo durante las pruebas de evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementación de la Red Neuronal\n",
        "\n",
        "La red neuronal es el \"cerebro\" de nuestro agente DQN. La arquitectura de red neuronal que hemos utilizado dispone de tres capas convolucionales (CNNs). Las redes neuronales convolucionales son especialmente eficaces para procesar imágenes, lo que es clave para nuestro agente en este proyecto, que debe interpretar y tomar decisiones basadas en la observación del estado del juego.\n",
        "\n",
        "1. La red neuronal recibe como entrada un conjunto de 4 cuadros de 84x84 píxeles (ventana temporal de 4), lo cual permite que el agente tenga en cuenta la temporalidad de los movimientos. Esto es clave para que el modelo entienda hacia dónde se mueven los disparos o el desplazamiento de los enemigos. Utilizamos un procesador personalizado (AtariProcessor) que convierte las imágenes en escala de grises y redimensiona las observaciones para resaltar las características relevantes.\n",
        "\n",
        "2. Capas convolucionales:\n",
        "\n",
        "- **Primera capa**: 32 filtros de 8x8 con strides de 4, seguidos de una activación ReLU. Esta capa captura características espaciales de bajo nivel como bordes y esquinas.\n",
        "- **Segunda capa**: 64 filtros de 4x4 con strides de 2, con activación ReLU, para capturar patrones más complejos.\n",
        "- **Tercera capa**: 64 filtros de 3x3 con activación ReLU para detectar características aún más abstractas.\n",
        "\n",
        "3. Capas densas:\n",
        "\n",
        "- **Primera capa densa**: 512 unidades con activación ReLU para integrar las características extraídas por las capas convolucionales.\n",
        "- **Segunda capa densa**: 256 unidades con activación ReLU para una mayor abstracción de la información.\n",
        "- **Capa de salida**: Dispone de tantas neuronas como acciones posibles hay en el juego, con activación lineal para predecir los valores Q de cada acción.\n",
        "\n",
        "Esta arquitectura permite que el agente capture información espacial y temporal del entorno, facilitando una toma de decisiones eficiente.\n",
        "\n",
        "### Componentes de la Solución DQN\n",
        "\n",
        "Tal y como se ha explicado en la cronología, la implementación de los parámetros del agente DQN es esencial para la mejora de los resultados. Aquí detallamos los componentes clave:\n",
        "\n",
        "1. Memoria secuencial: Utilizamos una memoria secuencial (SequentialMemory) con un límite de 100,000 transiciones. Esto permite almacenar experiencias pasadas y muestrearlas durante el entrenamiento, mejorando la estabilidad del entrenamiento.\n",
        "\n",
        "2. Política de exploración: Empleamos una política `LinearAnnealedPolicy` con `EpsGreedyQPolicy` para gestionar la correlación entre exploración y explotación, conceptos clave en cualquier entrenamiento de modelos de Aprendizaje por Refuerzo Profundo. La política comienza con una alta tasa de exploración (eps=1.0) que disminuye gradualmente a 0.1 durante 100.000 pasos, promoviendo la exploración inicial y la explotación en etapas posteriores.\n",
        "\n",
        "3. Configuración del agente DQN:\n",
        "\n",
        "- **nb_steps_warmup** de 100,000 pasos para una base sólida de experiencias antes de comenzar la actualización y entrenamiento de los pesos de la red.\n",
        "- **gamma** de 0.99 para valorar las recompensas futuras.\n",
        "- **target_model_update** cada 10,000 pasos para mayor estabilidad en las actualizaciones del modelo.\n",
        "- **train_interval** de 4 pasos para actualizar la red neuronal de manera eficiente sin sobrecargar el sistema.\n",
        "\n",
        "4. Habilitación de la red de duelo: Activar `dueling_network` con tipo 'avg' mejora la capacidad del agente para evaluar el valor de cada estado de forma más precisa, diferenciando entre el valor de estar en un estado y el valor de realizar una acción en dicho estado.\n",
        "\n",
        "Estos componentes permiten que el agente aprenda de manera eficiente y estable en nuestro entorno del proyecto 'Space Invaders'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bibliografía\n",
        "\n",
        "Como material adicional a las clases del curso, hemos buscado información relevante en los siguientes enlaces:\n",
        "\n",
        "- [Ventajas y desventajas del Aprendizaje por Refuerzo Profundo](https://huggingface.co/learn/deep-rl-course/unit4/advantages-disadvantages)\n",
        "- [Métodos basados en políticas](https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods)\n",
        "- [Atari Game en Kaggle](https://www.kaggle.com/code/elsall/atari-game)\n",
        "- [Mastering Atari Games: Deep Reinforcement Learning Unleashed](https://medium.com/@chemistry8526/mastering-atari-games-deep-reinforcement-learning-unleashed-f16af5e2144a)\n",
        "- [Space Invaders en Gymnasium](https://gymnasium.farama.org/environments/atari/space_invaders/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
